{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Dynamic Workflow Chatbot with CFG GenAI\n",
    "\n",
    "This notebook demonstrates an **intelligent, adaptive chatbot** using LangGraph with a dynamic workflow system that:\n",
    "\n",
    "## üéØ **Core Capabilities:**\n",
    "1. **Dynamic Tool Orchestration** - Intelligently decides execution path based on query analysis\n",
    "2. **Context-Aware Processing** - Handles follow-up questions with conversation memory\n",
    "3. **Smart Path Optimization** - Skips unnecessary tools for faster responses\n",
    "4. **Multi-table SQL Generation** - Complex queries across related database tables\n",
    "5. **Robust Error Handling** - Graceful fallbacks with structured data formatting\n",
    "6. **Session Management** - Tracks conversation history per user session\n",
    "\n",
    "## üöÄ **Dynamic Workflow Features:**\n",
    "- **Adaptive Routing**: Different execution paths for different query types\n",
    "- **Performance Optimization**: Skip tools when not needed\n",
    "- **LLM-Driven Decisions**: Workflow orchestrator analyzes and plans execution\n",
    "- **Context Intelligence**: Automatic follow-up question enhancement\n",
    "\n",
    "## üîß **Technology Stack:**\n",
    "- **LangGraph**: Dynamic workflow orchestration\n",
    "- **CFG GenAI**: Proprietary LLM integration\n",
    "- **Snowflake**: Enterprise data warehouse\n",
    "- **Python**: Core implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in your environment)\n",
    "# !pip install langgraph pandas python-dotenv snowflake-connector-python cfggenaisdk\n",
    "\n",
    "from snowflake.connector import DictCursor\n",
    "import snowflake.connector\n",
    "from cfggenaisdk import CFGGenAIGDK\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, TypedDict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CFG GenAI Integration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CFG GenAI GDK client\n",
    "USECASE_ID = os.getenv(\"CFG_USECASE_ID\", \"chatbot_usecase\")\n",
    "EXPERIMENT_NAME = os.getenv(\"CFG_EXPERIMENT_NAME\", \"langgraph_chatbot\")\n",
    "EXPERIMENT_DESC = os.getenv(\"CFG_EXPERIMENT_DESC\", \"LangGraph chatbot with CFG GenAI\")\n",
    "\n",
    "try:\n",
    "    gdk = CFGGenAIGDK(USECASE_ID, EXPERIMENT_NAME, EXPERIMENT_DESC)\n",
    "    print(\"‚úÖ CFG GenAI GDK initialized successfully!\")\n",
    "    print(f\"Usecase ID: {USECASE_ID}\")\n",
    "    print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize CFG GenAI GDK: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "def call_llm(prompt: str, system_prompt: str = \"\", model: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Robust helper function to make LLM calls using CFG GenAI GDK\n",
    "    Handles different response structures\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use model from environment variable if not specified\n",
    "        if model is None:\n",
    "            model = os.getenv(\"CFG_MODEL_ID\", \"md005_openai_gpt4o\")\n",
    "        \n",
    "        # Combine system prompt and user prompt\n",
    "        if system_prompt:\n",
    "            combined_prompt = f\"{system_prompt}\\n\\nUser Query: {prompt}\"\n",
    "        else:\n",
    "            combined_prompt = prompt\n",
    "        \n",
    "        # Prepare prompt template for CFG GenAI\n",
    "        prompt_template = {\n",
    "            \"prompt_template\": [\n",
    "                {\"role\": \"system\", \"content\": combined_prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Set hyperparameters\n",
    "        hyperparam = {\n",
    "            'max_tokens': 2000,\n",
    "            'temperature': 0.1\n",
    "        }\n",
    "        \n",
    "        # Make the LLM call\n",
    "        gdk_response = gdk.invoke_llmgateway(\n",
    "            prompt_template=prompt_template,\n",
    "            hyperparam=hyperparam,\n",
    "            model_id=model\n",
    "        )\n",
    "        \n",
    "        # Try different ways to extract the content based on response structure\n",
    "        generated_response = None\n",
    "        \n",
    "        # Method 1: Standard OpenAI-like structure\n",
    "        try:\n",
    "            if isinstance(gdk_response, dict):\n",
    "                generated_response = gdk_response['genResponse']['choices'][0]['message']['content']\n",
    "        except (KeyError, TypeError, IndexError):\n",
    "            pass\n",
    "        \n",
    "        # Method 2: Direct choices access\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, dict) and 'choices' in gdk_response:\n",
    "                    generated_response = gdk_response['choices'][0]['message']['content']\n",
    "            except (KeyError, TypeError, IndexError):\n",
    "                pass\n",
    "        \n",
    "        # Method 3: Direct response field\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, dict):\n",
    "                    if 'response' in gdk_response:\n",
    "                        generated_response = gdk_response['response']\n",
    "                    elif 'content' in gdk_response:\n",
    "                        generated_response = gdk_response['content']\n",
    "                    elif 'text' in gdk_response:\n",
    "                        generated_response = gdk_response['text']\n",
    "            except (KeyError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # Method 4: Tuple/List response\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, (list, tuple)) and len(gdk_response) > 0:\n",
    "                    first_element = gdk_response[0]\n",
    "                    if isinstance(first_element, str):\n",
    "                        generated_response = first_element\n",
    "                    elif isinstance(first_element, dict):\n",
    "                        if 'content' in first_element:\n",
    "                            generated_response = first_element['content']\n",
    "                        elif 'message' in first_element:\n",
    "                            generated_response = first_element['message']\n",
    "                        elif 'text' in first_element:\n",
    "                            generated_response = first_element['text']\n",
    "            except (IndexError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # Method 5: String response\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, str):\n",
    "                    generated_response = gdk_response\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # If all methods failed, return the raw response as string\n",
    "        if generated_response is None:\n",
    "            generated_response = str(gdk_response)\n",
    "        \n",
    "        return generated_response.strip() if generated_response else \"No response generated\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CFG GenAI LLM call failed: {e}\")\n",
    "        if 'gdk_response' in locals():\n",
    "            print(f\"Response type: {type(gdk_response)}\")\n",
    "            print(f\"Response: {gdk_response}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ CFG GenAI integration setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test CFG GenAI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the CFG GenAI integration\n",
    "test_response = call_llm(\n",
    "    prompt=\"What is 2 + 2?\",\n",
    "    system_prompt=\"You are a helpful assistant. Provide brief, accurate answers.\"\n",
    ")\n",
    "\n",
    "print(f\"üß™ Test Response: {test_response}\")\n",
    "print(\"‚úÖ CFG GenAI integration test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Schema and Snowflake Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseSchema:\n",
    "    \"\"\"\n",
    "    Load and manage database schema from external text file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema_file_path: str = \"database_schema.txt\"):\n",
    "        self.schema_file_path = schema_file_path\n",
    "        self.tables = {}\n",
    "        self.relationships = {}\n",
    "        self.raw_schema_text = \"\"\n",
    "        self.load_schema()\n",
    "\n",
    "    def load_schema(self):\n",
    "        try:\n",
    "            if os.path.exists(self.schema_file_path):\n",
    "                with open(self.schema_file_path, 'r', encoding='utf-8') as file:\n",
    "                    self.raw_schema_text = file.read()\n",
    "                print(f\"‚úÖ Database schema loaded from {self.schema_file_path}\")\n",
    "                self.parse_schema()\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Schema file {self.schema_file_path} not found. Please create this file with your database schema.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading schema file: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def parse_schema(self):\n",
    "        try:\n",
    "            lines = self.raw_schema_text.strip().split('\\n')\n",
    "            current_table = None\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                if line.upper().startswith('TABLE:') or line.upper().startswith('TABLE '):\n",
    "                    current_table = line.split(':', 1)[1].strip() if ':' in line else line.split(' ', 1)[1].strip()\n",
    "                    current_table = current_table.upper()\n",
    "                    self.tables[current_table] = {\n",
    "                        'description': f\"Table: {current_table}\",\n",
    "                        'columns': {}\n",
    "                    }\n",
    "\n",
    "                elif current_table and ('|' in line or '\\t' in line or '  ' in line):\n",
    "                    parts = []\n",
    "                    if '|' in line:\n",
    "                        parts = [part.strip() for part in line.split('|')]\n",
    "                    elif '\\t' in line:\n",
    "                        parts = [part.strip() for part in line.split('\\t') if part.strip()]\n",
    "                    else:\n",
    "                        parts = [part.strip() for part in line.split() if part.strip()]\n",
    "\n",
    "                    if len(parts) >= 2:\n",
    "                        col_name = parts[0].upper()\n",
    "                        col_type = parts[1].upper()\n",
    "                        col_description = parts[2] if len(parts) > 2 else f\"{col_name} column\"\n",
    "\n",
    "                        self.tables[current_table]['columns'][col_name] = {\n",
    "                            'type': col_type,\n",
    "                            'description': col_description\n",
    "                        }\n",
    "\n",
    "            print(f\"‚úÖ Parsed {len(self.tables)} tables from schema file\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing schema: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def get_schema_description(self) -> str:\n",
    "        if self.raw_schema_text and len(self.tables) > 1:\n",
    "            schema_text = \"DATABASE SCHEMA:\\n\\n\"\n",
    "            schema_text += self.raw_schema_text\n",
    "            schema_text += \"\\n\\nAVAILABLE TABLES:\\n\"\n",
    "            for table_name in self.tables.keys():\n",
    "                schema_text += f\"- {table_name}\\n\"\n",
    "            return schema_text\n",
    "        else:\n",
    "            schema_text = \"DATABASE SCHEMA:\\n\\n\"\n",
    "            for table_name, table_info in self.tables.items():\n",
    "                schema_text += f\"Table: {table_name}\\n\"\n",
    "                schema_text += f\"Description: {table_info['description']}\\n\"\n",
    "                schema_text += \"Columns:\\n\"\n",
    "                for col_name, col_info in table_info['columns'].items():\n",
    "                    schema_text += f\"  - {col_name} ({col_info['type']}): {col_info['description']}\\n\"\n",
    "                schema_text += \"\\n\"\n",
    "            return schema_text\n",
    "\n",
    "    def get_table_names(self) -> List[str]:\n",
    "        return list(self.tables.keys())\n",
    "\n",
    "\n",
    "class SnowflakeConfig:\n",
    "    def __init__(self):\n",
    "        self.account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "        self.user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "        self.password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "        self.warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\n",
    "        self.database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "        self.schema = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n",
    "        self.role = os.getenv(\"SNOWFLAKE_ROLE\", \"ACCOUNTADMIN\")\n",
    "\n",
    "        required_fields = {\n",
    "            'account': self.account,\n",
    "            'user': self.user,\n",
    "            'password': self.password,\n",
    "            'database': self.database\n",
    "        }\n",
    "\n",
    "        missing_fields = [field for field, value in required_fields.items() if not value]\n",
    "        if missing_fields:\n",
    "            raise ValueError(\n",
    "                f\"Missing required Snowflake configuration: {', '.join(missing_fields)}. Please check your .env file.\")\n",
    "\n",
    "\n",
    "# Global config instances\n",
    "snowflake_config = None\n",
    "database_schema = None\n",
    "\n",
    "\n",
    "def initialize_configs():\n",
    "    global snowflake_config, database_schema\n",
    "    if snowflake_config is None:\n",
    "        snowflake_config = SnowflakeConfig()\n",
    "    if database_schema is None:\n",
    "        database_schema = DatabaseSchema()\n",
    "    return snowflake_config, database_schema\n",
    "\n",
    "\n",
    "def get_snowflake_connection():\n",
    "    global snowflake_config\n",
    "    if snowflake_config is None:\n",
    "        snowflake_config = SnowflakeConfig()\n",
    "\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            account=snowflake_config.account,\n",
    "            user=snowflake_config.user,\n",
    "            password=snowflake_config.password,\n",
    "            warehouse=snowflake_config.warehouse,\n",
    "            database=snowflake_config.database,\n",
    "            schema=snowflake_config.schema,\n",
    "            role=snowflake_config.role\n",
    "        )\n",
    "        print(\"‚úÖ Snowflake connection established successfully!\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to Snowflake: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def execute_snowflake_query(sql_query: str) -> Dict[str, Any]:\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_snowflake_connection()\n",
    "        cursor = conn.cursor(DictCursor)\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "\n",
    "        print(f\"üìä Query executed successfully. Returned {len(results)} rows.\")\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": results,\n",
    "            \"columns\": columns,\n",
    "            \"row_count\": len(results)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing Snowflake query: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"data\": [],\n",
    "            \"columns\": [],\n",
    "            \"row_count\": 0\n",
    "        }\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Database configuration setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configurations on startup to validate setup\n",
    "try:\n",
    "    initialize_configs()\n",
    "    print(\"‚úÖ All configurations initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Configuration initialization failed: {e}\")\n",
    "    print(\"Please check your .env file and database_schema.txt file before running queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. State Management and Session Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    query: str\n",
    "    response: str\n",
    "    sql_query: str\n",
    "    query_result: Dict[str, Any]\n",
    "    timestamp: datetime\n",
    "    intent: str\n",
    "\n",
    "\n",
    "class SessionManager:\n",
    "    def __init__(self):\n",
    "        self.sessions: Dict[str, List[ConversationTurn]] = {}\n",
    "        self.max_history = 5\n",
    "\n",
    "    def add_turn(self, session_id: str, turn: ConversationTurn):\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = []\n",
    "        self.sessions[session_id].append(turn)\n",
    "        if len(self.sessions[session_id]) > self.max_history:\n",
    "            self.sessions[session_id] = self.sessions[session_id][-self.max_history:]\n",
    "\n",
    "    def get_conversation_context(self, session_id: str) -> str:\n",
    "        if session_id not in self.sessions or not self.sessions[session_id]:\n",
    "            return \"No previous conversation history.\"\n",
    "\n",
    "        context = \"PREVIOUS CONVERSATION HISTORY:\\n\"\n",
    "        for i, turn in enumerate(self.sessions[session_id], 1):\n",
    "            context += f\"\\n{i}. User: {turn.query}\"\n",
    "            context += f\"\\n   Response: {turn.response}\"\n",
    "            if turn.sql_query:\n",
    "                context += f\"\\n   SQL Used: {turn.sql_query}\"\n",
    "\n",
    "        context += \"\\n\\nUse this context to understand follow-up questions and references like 'that', 'those', 'compare to previous', etc.\"\n",
    "        return context\n",
    "\n",
    "    def clear_session(self, session_id: str):\n",
    "        if session_id in self.sessions:\n",
    "            del self.sessions[session_id]\n",
    "\n",
    "\n",
    "class WorkflowState(TypedDict):\n",
    "    session_id: str\n",
    "    user_query: str\n",
    "    original_query: str\n",
    "    intent: str\n",
    "    is_relevant: bool\n",
    "    is_followup: bool\n",
    "    sql_query: str\n",
    "    query_result: Dict[str, Any]\n",
    "    final_response: str\n",
    "    error: str\n",
    "    context: Dict[str, Any]\n",
    "    conversation_context: str\n",
    "    # Dynamic workflow tracking\n",
    "    context_enhanced: bool\n",
    "    orchestration_completed: bool\n",
    "    intent_classified: bool\n",
    "    sql_generated: bool\n",
    "    sql_executed: bool\n",
    "    response_formatted: bool\n",
    "\n",
    "\n",
    "# Global session manager\n",
    "session_manager = SessionManager()\n",
    "\n",
    "\n",
    "def initialize_state(session_id: str, user_query: str) -> WorkflowState:\n",
    "    conversation_context = session_manager.get_conversation_context(session_id)\n",
    "\n",
    "    return WorkflowState(\n",
    "        session_id=session_id,\n",
    "        user_query=user_query,\n",
    "        original_query=user_query,\n",
    "        intent=\"\",\n",
    "        is_relevant=False,\n",
    "        is_followup=False,\n",
    "        sql_query=\"\",\n",
    "        query_result={},\n",
    "        final_response=\"\",\n",
    "        error=\"\",\n",
    "        context={},\n",
    "        conversation_context=conversation_context,\n",
    "        # Initialize completion tracking\n",
    "        context_enhanced=False,\n",
    "        orchestration_completed=False,\n",
    "        intent_classified=False,\n",
    "        sql_generated=False,\n",
    "        sql_executed=False,\n",
    "        response_formatted=False\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ State management and session handling setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dynamic Workflow Orchestrator - The Brain of the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow_orchestrator_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    üß† Dynamic workflow orchestrator that decides execution strategy and tool sequence\n",
    "    This is the core intelligence that makes the workflow adaptive!\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Orchestrating dynamic workflow for: {state['user_query']}\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a dynamic workflow orchestrator for a data analysis chatbot.\n",
    "\n",
    "Available tools and their purposes:\n",
    "1. context_enhancer - Enhances follow-up questions with previous conversation context\n",
    "2. intent_classifier - Determines if query is relevant to business data analysis\n",
    "3. nl_to_sql - Converts natural language to SQL queries\n",
    "4. sql_executor - Executes SQL queries against Snowflake database\n",
    "5. response_formatter - Formats query results into natural language\n",
    "\n",
    "Analyze the user query and determine the optimal execution path. Consider:\n",
    "- Is this a follow-up question that needs context enhancement?\n",
    "- Is the query obviously relevant to data analysis (skip intent classification)?\n",
    "- What's the query complexity and expected processing needs?\n",
    "- Are there any shortcuts or optimizations possible?\n",
    "\n",
    "Respond with a JSON object containing the execution plan:\n",
    "{\n",
    "    \"execution_path\": [\"tool1\", \"tool2\", \"tool3\"],\n",
    "    \"skip_tools\": [\"tool_name\"],\n",
    "    \"reasoning\": \"Why this path was chosen\",\n",
    "    \"complexity\": \"simple|moderate|complex\",\n",
    "    \"query_type\": \"greeting|irrelevant|simple_data|complex_data|follow_up\",\n",
    "    \"optimizations\": [\"optimization1\", \"optimization2\"]\n",
    "}\n",
    "\n",
    "Example paths:\n",
    "- Simple data query: [\"intent_classifier\", \"nl_to_sql\", \"sql_executor\", \"response_formatter\"]\n",
    "- Follow-up question: [\"context_enhancer\", \"nl_to_sql\", \"sql_executor\", \"response_formatter\"]\n",
    "- Greeting/irrelevant: [\"intent_classifier\"]\n",
    "- Obviously relevant complex query: [\"nl_to_sql\", \"sql_executor\", \"response_formatter\"]\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Plan execution path for: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        # Try to parse the JSON response\n",
    "        try:\n",
    "            import json\n",
    "            workflow_plan = json.loads(llm_response)\n",
    "            state['context']['workflow_plan'] = workflow_plan\n",
    "            state['context']['execution_path'] = workflow_plan.get('execution_path', [])\n",
    "            state['context']['skip_tools'] = workflow_plan.get('skip_tools', [])\n",
    "            \n",
    "            print(f\"üìã Dynamic Workflow Plan:\")\n",
    "            print(f\"   Execution Path: {workflow_plan.get('execution_path', [])}\")\n",
    "            print(f\"   Skip Tools: {workflow_plan.get('skip_tools', [])}\")\n",
    "            print(f\"   Reasoning: {workflow_plan.get('reasoning', 'No reasoning provided')}\")\n",
    "            print(f\"   Complexity: {workflow_plan.get('complexity', 'unknown')}\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback to default path\n",
    "            default_path = [\"intent_classifier\", \"nl_to_sql\", \"sql_executor\", \"response_formatter\"]\n",
    "            state['context']['workflow_plan'] = {\n",
    "                \"execution_path\": default_path,\n",
    "                \"skip_tools\": [],\n",
    "                \"reasoning\": \"Default path due to parsing error\",\n",
    "                \"complexity\": \"simple\",\n",
    "                \"query_type\": \"simple_data\"\n",
    "            }\n",
    "            state['context']['execution_path'] = default_path\n",
    "            state['context']['skip_tools'] = []\n",
    "            print(f\"‚ö†Ô∏è Using default workflow path due to parsing error: {default_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error in workflow orchestration: {str(e)}\"\n",
    "        print(f\"‚ùå Error in workflow orchestration: {e}\")\n",
    "\n",
    "    # Mark completion\n",
    "    state['orchestration_completed'] = True\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"‚úÖ Dynamic workflow orchestrator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Core Processing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enhancer_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    üîó Enhance user query with conversation context for follow-up questions\n",
    "    \"\"\"\n",
    "    print(f\"üîó Enhancing query with context: {state['user_query']}\")\n",
    "\n",
    "    if state['conversation_context'] == \"No previous conversation history.\":\n",
    "        print(\"‚úÖ No previous context - using original query\")\n",
    "        state['context_enhanced'] = True\n",
    "        return state\n",
    "\n",
    "    system_prompt = f\"\"\"You are a context enhancement assistant for a business data analysis chatbot.\n",
    "\n",
    "{state['conversation_context']}\n",
    "\n",
    "Your job is to:\n",
    "1. Determine if the current user query is a follow-up question that references previous conversation\n",
    "2. If it's a follow-up, enhance the query with proper context to make it standalone\n",
    "3. If it's not a follow-up, return the original query unchanged\n",
    "\n",
    "Follow-up indicators include:\n",
    "- References like \"that\", \"those\", \"it\", \"them\"\n",
    "- Comparative phrases like \"compared to that\", \"vs the previous\", \"how about\"\n",
    "- Continuation phrases like \"what about\", \"and for\", \"also show me\"\n",
    "- Time references building on previous queries like \"and last month\", \"for the same period\"\n",
    "\n",
    "IMPORTANT: \n",
    "- If it's a follow-up, rewrite the query to be completely standalone and clear\n",
    "- If it's NOT a follow-up, return exactly: \"NOT_FOLLOWUP: [original query]\"\n",
    "- If it IS a follow-up, return: \"FOLLOWUP: [enhanced standalone query]\"\n",
    "\n",
    "Examples:\n",
    "- \"What about premium customers?\" ‚Üí \"FOLLOWUP: What are the sales metrics for premium customers?\"\n",
    "- \"Compare that to last month\" ‚Üí \"FOLLOWUP: Compare the transaction volume from the previous query to last month's transaction volume\"\n",
    "- \"Show me product sales\" ‚Üí \"NOT_FOLLOWUP: Show me product sales\"\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Current user query: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        print(f\"ü§ñ Context Enhancement Response: {llm_response}\")\n",
    "\n",
    "        if llm_response.startswith(\"FOLLOWUP:\"):\n",
    "            enhanced_query = llm_response.replace(\"FOLLOWUP:\", \"\").strip()\n",
    "            state['user_query'] = enhanced_query\n",
    "            state['is_followup'] = True\n",
    "            print(f\"‚úÖ Enhanced follow-up query: {enhanced_query}\")\n",
    "        else:\n",
    "            state['is_followup'] = False\n",
    "            print(\"‚úÖ Not a follow-up question - using original query\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error in context enhancement: {str(e)}\"\n",
    "        print(f\"‚ùå Error in context enhancement: {e}\")\n",
    "\n",
    "    state['context_enhanced'] = True\n",
    "    return state\n",
    "\n",
    "\n",
    "def intent_classifier_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    üîç Determine if the user query is relevant to our use case using LLM\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyzing intent for: {state['user_query']}\")\n",
    "\n",
    "    system_prompt = \"\"\"You are an intent classifier for a comprehensive business data analysis system.\n",
    "    \n",
    "    Your job is to determine if a user query is relevant to business data analysis using our available tables:\n",
    "    - TRANSACTIONS: Transaction records with volumes, dates, customer and product IDs\n",
    "    - CUSTOMERS: Customer information including segments, locations, registration dates\n",
    "    - PRODUCTS: Product catalog with categories, prices, brands, suppliers\n",
    "    \n",
    "    Relevant queries include:\n",
    "    - Transaction analysis (volumes, amounts, counts, trends)\n",
    "    - Customer analysis (segments, behavior, demographics, registration patterns)\n",
    "    - Product analysis (categories, performance, pricing, brand analysis)\n",
    "    - Sales and revenue analysis across any dimension\n",
    "    - Time-based queries (last Friday, yesterday, this week, monthly trends, etc.)\n",
    "    - Comparative analysis (compare periods, segments, products, customers)\n",
    "    - Cross-table analysis (customer transaction patterns, product performance by segment, etc.)\n",
    "    - Data aggregation requests (total, sum, average, count, etc.)\n",
    "    - Business intelligence queries combining multiple data sources\n",
    "    \n",
    "    Irrelevant queries include:\n",
    "    - General conversation, greetings\n",
    "    - Questions about weather, news, personal topics\n",
    "    - Technical support unrelated to data\n",
    "    - Requests for information outside of our business data domain\n",
    "    \n",
    "    Respond with ONLY one of these formats:\n",
    "    RELEVANT: data_query\n",
    "    IRRELEVANT: general_conversation\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Classify this user query: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        print(f\"ü§ñ LLM Intent Response: {llm_response}\")\n",
    "\n",
    "        if \"RELEVANT\" in llm_response.upper():\n",
    "            state['intent'] = \"data_query\"\n",
    "            state['is_relevant'] = True\n",
    "            print(\"‚úÖ Query is relevant - proceeding with data analysis\")\n",
    "        else:\n",
    "            state['intent'] = \"irrelevant\"\n",
    "            state['is_relevant'] = False\n",
    "            state['final_response'] = \"I'm sorry, but I can only help with transaction and data-related queries. Please ask about transaction volumes, sales data, or similar topics.\"\n",
    "            print(\"‚ùå Query is not relevant to our use case\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error in intent classification: {str(e)}\"\n",
    "        print(f\"‚ùå Error in intent classification: {e}\")\n",
    "\n",
    "    state['intent_classified'] = True\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"‚úÖ Context and Intent tools defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_to_sql_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    üîÑ Convert natural language to SQL using LLM with multi-table support\n",
    "    \"\"\"\n",
    "    if not state['is_relevant']:\n",
    "        state['sql_generated'] = True\n",
    "        return state\n",
    "\n",
    "    print(f\"üîÑ Converting to SQL: {state['user_query']}\")\n",
    "\n",
    "    # Get current date for context\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    current_day = datetime.now().strftime('%A')\n",
    "\n",
    "    # Initialize configs and get the comprehensive schema description\n",
    "    global snowflake_config, database_schema\n",
    "    if snowflake_config is None:\n",
    "        snowflake_config = SnowflakeConfig()\n",
    "    if database_schema is None:\n",
    "        database_schema = DatabaseSchema()\n",
    "\n",
    "    schema_description = database_schema.get_schema_description()\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert SQL query generator for a Snowflake database with multiple related tables.\n",
    "\n",
    "{schema_description}\n",
    "\n",
    "Database Configuration:\n",
    "- Database: {snowflake_config.database}\n",
    "- Schema: {snowflake_config.schema}\n",
    "- Current date: {current_date} ({current_day})\n",
    "\n",
    "SNOWFLAKE SQL GENERATION RULES:\n",
    "\n",
    "1. QUERY STRUCTURE:\n",
    "   - Generate ONLY the SQL query, no explanations or markdown\n",
    "   - Use fully qualified table names: {snowflake_config.database}.{snowflake_config.schema}.TABLE_NAME\n",
    "   - Always use uppercase for SQL keywords and table/column names\n",
    "\n",
    "2. MULTI-TABLE QUERIES:\n",
    "   - Use JOINs when the query requires data from multiple tables\n",
    "   - Common patterns:\n",
    "     * Customer analysis: JOIN TRANSACTIONS with CUSTOMERS\n",
    "     * Product analysis: JOIN TRANSACTIONS with PRODUCTS  \n",
    "     * Complete analysis: JOIN all three tables\n",
    "   - Use appropriate JOIN types (INNER JOIN for existing relationships)\n",
    "\n",
    "3. DATE FUNCTIONS (Snowflake-specific):\n",
    "   - Current date: CURRENT_DATE()\n",
    "   - Yesterday: DATEADD('day', -1, CURRENT_DATE())\n",
    "   - Last Friday: DATEADD('day', -1, DATE_TRUNC('week', CURRENT_DATE()) + 4)\n",
    "   - Last week: DATE_TRUNC('week', DATEADD('week', -1, CURRENT_DATE()))\n",
    "   - This month: DATE_TRUNC('month', CURRENT_DATE())\n",
    "   - Last month: DATE_TRUNC('month', DATEADD('month', -1, CURRENT_DATE()))\n",
    "\n",
    "4. AGGREGATIONS:\n",
    "   - Use SUM() for volume/amount calculations\n",
    "   - Use COUNT() for transaction counts\n",
    "   - Use AVG() for averages\n",
    "   - Use GROUP BY for breakdowns by categories, dates, etc.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Convert this natural language query to SQL: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        # Clean up the response to extract just the SQL\n",
    "        sql_query = llm_response.strip()\n",
    "\n",
    "        # Remove any markdown formatting or extra text\n",
    "        if \"```sql\" in sql_query:\n",
    "            sql_query = sql_query.split(\"```sql\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in sql_query:\n",
    "            sql_query = sql_query.split(\"```\")[1].strip()\n",
    "\n",
    "        # Remove any trailing semicolon and clean up\n",
    "        sql_query = sql_query.rstrip(';').strip()\n",
    "\n",
    "        state['sql_query'] = sql_query\n",
    "        print(f\"üìù Generated SQL: {state['sql_query']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error generating SQL: {str(e)}\"\n",
    "        print(f\"‚ùå Error in SQL generation: {e}\")\n",
    "\n",
    "    state['sql_generated'] = True\n",
    "    return state\n",
    "\n",
    "\n",
    "def sql_executor_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    ‚ö° Execute the generated SQL query against Snowflake database\n",
    "    \"\"\"\n",
    "    if not state['is_relevant'] or state['error'] or not state['sql_query']:\n",
    "        state['sql_executed'] = True\n",
    "        return state\n",
    "\n",
    "    print(f\"‚ö° Executing SQL against Snowflake: {state['sql_query']}\")\n",
    "\n",
    "    try:\n",
    "        # Execute the query against Snowflake\n",
    "        query_result = execute_snowflake_query(state['sql_query'])\n",
    "\n",
    "        if query_result['success']:\n",
    "            # Process the results into a format suitable for response generation\n",
    "            data = query_result['data']\n",
    "\n",
    "            if not data:\n",
    "                state['query_result'] = {'message': 'No data found for the specified criteria'}\n",
    "            elif len(data) == 1:\n",
    "                # Single row result\n",
    "                state['query_result'] = dict(data[0])\n",
    "            else:\n",
    "                # Multiple rows - convert to a more readable format\n",
    "                # Configuration for result limits\n",
    "                max_full_results = int(os.getenv(\"MAX_FULL_RESULTS\", \"50\"))\n",
    "                max_sample_results = int(os.getenv(\"MAX_SAMPLE_RESULTS\", \"20\"))\n",
    "                \n",
    "                if len(data) <= max_full_results:\n",
    "                    state['query_result'] = {\n",
    "                        'results': [dict(row) for row in data],\n",
    "                        'row_count': len(data)\n",
    "                    }\n",
    "                else:\n",
    "                    # For large result sets, provide sample\n",
    "                    state['query_result'] = {\n",
    "                        'sample_results': [dict(row) for row in data[:max_sample_results]],\n",
    "                        'total_rows': len(data),\n",
    "                        'message': f'Showing first {max_sample_results} of {len(data)} results'\n",
    "                    }\n",
    "\n",
    "            print(f\"üìä Query executed successfully. Result: {state['query_result']}\")\n",
    "\n",
    "        else:\n",
    "            # Query failed\n",
    "            state['error'] = f\"Snowflake query failed: {query_result['error']}\"\n",
    "            print(f\"‚ùå Snowflake query failed: {query_result['error']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error executing SQL: {str(e)}\"\n",
    "        print(f\"‚ùå Error in SQL execution: {e}\")\n",
    "\n",
    "    state['sql_executed'] = True\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"‚úÖ SQL generation and execution tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Enhanced Response Formatter with Table Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results_as_table(results: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    üìä Format query results as a simple table string\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return \"No data available\"\n",
    "\n",
    "    try:\n",
    "        # Get all unique keys from all results\n",
    "        all_keys = set()\n",
    "        for result in results:\n",
    "            all_keys.update(result.keys())\n",
    "\n",
    "        headers = list(all_keys)\n",
    "\n",
    "        # Create header row\n",
    "        table_str = \" | \".join(headers) + \"\\n\"\n",
    "        table_str += \"-\" * len(table_str) + \"\\n\"\n",
    "\n",
    "        # Add data rows\n",
    "        for result in results:\n",
    "            row_values = []\n",
    "            for header in headers:\n",
    "                value = result.get(header, \"\")\n",
    "                # Format numbers with commas if they're large\n",
    "                if isinstance(value, (int, float)) and abs(value) >= 1000:\n",
    "                    value = f\"{value:,}\"\n",
    "                row_values.append(str(value))\n",
    "            table_str += \" | \".join(row_values) + \"\\n\"\n",
    "\n",
    "        return table_str\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback to simple string representation\n",
    "        return \"\\n\".join([str(result) for result in results])\n",
    "\n",
    "\n",
    "def format_single_result(result: Dict) -> str:\n",
    "    \"\"\"\n",
    "    üìã Format a single result dictionary as a readable string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        formatted_lines = []\n",
    "        for key, value in result.items():\n",
    "            # Format numbers with commas if they're large\n",
    "            if isinstance(value, (int, float)) and abs(value) >= 1000:\n",
    "                value = f\"{value:,}\"\n",
    "            formatted_lines.append(f\"{key}: {value}\")\n",
    "\n",
    "        return \"\\n\".join(formatted_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        return str(result)\n",
    "\n",
    "\n",
    "def response_formatter_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    üìù Format the query results into natural language response using LLM\n",
    "    üõ°Ô∏è If formatting fails, return the raw dataframe/results as structured tables\n",
    "    \"\"\"\n",
    "    if not state['is_relevant'] or state['error']:\n",
    "        state['response_formatted'] = True\n",
    "        return state\n",
    "\n",
    "    print(\"üìù Formatting response to natural language\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a data analyst assistant that converts query results into clear, natural language responses.\n",
    "\n",
    "Guidelines:\n",
    "1. Be conversational and helpful\n",
    "2. Include specific numbers with proper formatting (commas for thousands)\n",
    "3. For comparisons, calculate and mention percentage changes\n",
    "4. Use clear date references\n",
    "5. Keep responses concise but informative\n",
    "6. If showing multiple data points, organize them clearly\n",
    "\n",
    "Examples:\n",
    "- Single value: \"The total transaction volume for last Friday was 123,456.\"\n",
    "- Comparison: \"Transaction volume increased from 110,000 on August 22nd to 123,456 on August 29th, representing a 12.2% increase.\"\n",
    "- Multiple values: \"Here are the recent transaction volumes: August 29th: 123,456, August 28th: 98,765\"\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Prepare the data context for the LLM\n",
    "        data_context = f\"\"\"\n",
    "Original Query: {state['user_query']}\n",
    "SQL Query Used: {state['sql_query']}\n",
    "Query Results: {json.dumps(state['query_result'], indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Convert this query result into a natural language response:\\n\\n{data_context}\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        # Check if the LLM response is valid and not an error\n",
    "        if llm_response and not llm_response.startswith(\"Error:\") and len(llm_response.strip()) > 0:\n",
    "            state['final_response'] = llm_response.strip()\n",
    "            print(f\"‚úÖ Final response: {state['final_response']}\")\n",
    "        else:\n",
    "            raise ValueError(\"LLM returned invalid or empty response\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error in response formatting: {e}\")\n",
    "        print(\"üìä Returning raw data instead of formatted response\")\n",
    "\n",
    "        # Return the raw dataframe/results when formatting fails\n",
    "        query_result = state.get('query_result', {})\n",
    "\n",
    "        if isinstance(query_result, dict):\n",
    "            if 'results' in query_result:\n",
    "                # Multiple rows - convert to DataFrame-like string representation\n",
    "                results = query_result['results']\n",
    "                if results:\n",
    "                    # Create a simple table representation\n",
    "                    df_str = format_results_as_table(results)\n",
    "                    state['final_response'] = f\"Here are the query results:\\n\\n{df_str}\"\n",
    "                else:\n",
    "                    state['final_response'] = \"No data found for your query.\"\n",
    "            elif 'sample_results' in query_result:\n",
    "                # Large result set - show sample\n",
    "                sample_results = query_result['sample_results']\n",
    "                total_rows = query_result.get('total_rows', len(sample_results))\n",
    "                df_str = format_results_as_table(sample_results)\n",
    "                state['final_response'] = f\"Here are the first {len(sample_results)} results out of {total_rows} total:\\n\\n{df_str}\"\n",
    "            elif 'message' in query_result:\n",
    "                # No data message\n",
    "                state['final_response'] = query_result['message']\n",
    "            else:\n",
    "                # Single row or simple result\n",
    "                df_str = format_single_result(query_result)\n",
    "                state['final_response'] = f\"Query result:\\n\\n{df_str}\"\n",
    "        else:\n",
    "            # Fallback for unexpected result format\n",
    "            state['final_response'] = f\"Query completed. Result: {str(query_result)}\"\n",
    "\n",
    "        print(f\"üìã Raw data response: {state['final_response'][:200]}...\")\n",
    "\n",
    "    state['response_formatted'] = True\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"‚úÖ Enhanced response formatter with table support defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dynamic LangGraph Workflow Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_workflow():\n",
    "    \"\"\"\n",
    "    üöÄ Create a dynamic LangGraph workflow that adapts based on orchestrator decisions\n",
    "    This is where the magic happens - the workflow changes based on query analysis!\n",
    "    \"\"\"\n",
    "    # Create a new state graph\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "\n",
    "    # Add all available tool nodes\n",
    "    workflow.add_node(\"context_enhancer\", context_enhancer_tool)\n",
    "    workflow.add_node(\"workflow_orchestrator\", workflow_orchestrator_tool)\n",
    "    workflow.add_node(\"intent_classifier\", intent_classifier_tool)\n",
    "    workflow.add_node(\"nl_to_sql\", nl_to_sql_tool)\n",
    "    workflow.add_node(\"sql_executor\", sql_executor_tool)\n",
    "    workflow.add_node(\"response_formatter\", response_formatter_tool)\n",
    "\n",
    "    # Always start with context enhancement, then orchestration\n",
    "    workflow.set_entry_point(\"context_enhancer\")\n",
    "    workflow.add_edge(\"context_enhancer\", \"workflow_orchestrator\")\n",
    "\n",
    "    # Dynamic routing function based on orchestrator decisions\n",
    "    def dynamic_router(state: WorkflowState) -> str:\n",
    "        \"\"\"üß≠ Route to next tool based on orchestrator's execution plan\"\"\"\n",
    "        if state['error']:\n",
    "            return END\n",
    "            \n",
    "        execution_path = state['context'].get('execution_path', [])\n",
    "        skip_tools = state['context'].get('skip_tools', [])\n",
    "        \n",
    "        if not execution_path:\n",
    "            print(\"‚ö†Ô∏è No execution path found, using default path\")\n",
    "            execution_path = [\"intent_classifier\", \"nl_to_sql\", \"sql_executor\", \"response_formatter\"]\n",
    "            \n",
    "        # Determine what tools have been completed\n",
    "        completed_tools = set()\n",
    "        if state.get('context_enhanced', False):\n",
    "            completed_tools.add('context_enhancer')\n",
    "        if state.get('orchestration_completed', False):\n",
    "            completed_tools.add('workflow_orchestrator')\n",
    "        if state.get('intent_classified', False):\n",
    "            completed_tools.add('intent_classifier')\n",
    "        if state.get('sql_generated', False):\n",
    "            completed_tools.add('nl_to_sql')\n",
    "        if state.get('sql_executed', False):\n",
    "            completed_tools.add('sql_executor')\n",
    "        if state.get('response_formatted', False):\n",
    "            completed_tools.add('response_formatter')\n",
    "            \n",
    "        print(f\"üîç Completed tools: {completed_tools}\")\n",
    "        print(f\"üéØ Planned execution path: {execution_path}\")\n",
    "        print(f\"‚è≠Ô∏è Skip tools: {skip_tools}\")\n",
    "            \n",
    "        # Find next tool to execute\n",
    "        for tool in execution_path:\n",
    "            if tool not in completed_tools and tool not in skip_tools:\n",
    "                print(f\"‚û°Ô∏è Dynamic routing to: {tool}\")\n",
    "                return tool\n",
    "                \n",
    "        # Special handling for irrelevant queries\n",
    "        if state.get('intent_classified', False) and not state.get('is_relevant', True):\n",
    "            print(\"üö´ Query not relevant, ending workflow\")\n",
    "            return END\n",
    "                \n",
    "        # If all tools completed or no more tools, end workflow\n",
    "        print(\"‚úÖ All planned tools completed, ending workflow\")\n",
    "        return END\n",
    "\n",
    "    # Add dynamic conditional edges from orchestrator\n",
    "    workflow.add_conditional_edges(\n",
    "        \"workflow_orchestrator\",\n",
    "        dynamic_router,\n",
    "        {\n",
    "            \"context_enhancer\": \"context_enhancer\",\n",
    "            \"intent_classifier\": \"intent_classifier\", \n",
    "            \"nl_to_sql\": \"nl_to_sql\",\n",
    "            \"sql_executor\": \"sql_executor\",\n",
    "            \"response_formatter\": \"response_formatter\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add dynamic routing from each tool\n",
    "    for tool_name in [\"intent_classifier\", \"nl_to_sql\", \"sql_executor\"]:\n",
    "        workflow.add_conditional_edges(\n",
    "            tool_name,\n",
    "            dynamic_router,\n",
    "            {\n",
    "                \"context_enhancer\": \"context_enhancer\",\n",
    "                \"intent_classifier\": \"intent_classifier\",\n",
    "                \"nl_to_sql\": \"nl_to_sql\", \n",
    "                \"sql_executor\": \"sql_executor\",\n",
    "                \"response_formatter\": \"response_formatter\",\n",
    "                END: END\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Response formatter always ends\n",
    "    workflow.add_edge(\"response_formatter\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "    return app\n",
    "\n",
    "\n",
    "# Create our dynamic workflow\n",
    "chatbot_workflow = create_dynamic_workflow()\n",
    "print(\"üéâ Dynamic LangGraph workflow created successfully!\")\n",
    "print(\"üß† The workflow will now adapt its execution path based on query analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Chatbot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query: str, session_id: str = \"default\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    üöÄ Main function to process a user query through our dynamic LangGraph workflow\n",
    "\n",
    "    Args:\n",
    "        user_query: The user's natural language query\n",
    "        session_id: Session identifier for context management\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the final response and execution details\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Processing query with DYNAMIC WORKFLOW: '{user_query}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize state\n",
    "    initial_state = initialize_state(session_id, user_query)\n",
    "\n",
    "    try:\n",
    "        # Run the dynamic workflow\n",
    "        final_state = chatbot_workflow.invoke(initial_state)\n",
    "\n",
    "        # Prepare response\n",
    "        response = {\n",
    "            \"query\": user_query,\n",
    "            \"original_query\": final_state.get('original_query', user_query),\n",
    "            \"enhanced_query\": final_state.get('user_query', user_query),\n",
    "            \"is_followup\": final_state.get('is_followup', False),\n",
    "            \"response\": final_state.get('final_response', 'No response generated'),\n",
    "            \"session_id\": session_id,\n",
    "            \"success\": not bool(final_state.get('error')),\n",
    "            \"error\": final_state.get('error', ''),\n",
    "            \"execution_details\": {\n",
    "                \"intent\": final_state.get('intent', ''),\n",
    "                \"is_relevant\": final_state.get('is_relevant', False),\n",
    "                \"sql_query\": final_state.get('sql_query', ''),\n",
    "                \"query_result\": final_state.get('query_result', {}),\n",
    "                \"workflow_plan\": final_state.get('context', {}).get('workflow_plan', {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save conversation turn to session history (only if successful and relevant)\n",
    "        if response['success'] and final_state.get('is_relevant', False):\n",
    "            conversation_turn = ConversationTurn(\n",
    "                query=final_state.get('original_query', user_query),\n",
    "                response=final_state.get('final_response', ''),\n",
    "                sql_query=final_state.get('sql_query', ''),\n",
    "                query_result=final_state.get('query_result', {}),\n",
    "                timestamp=datetime.now(),\n",
    "                intent=final_state.get('intent', '')\n",
    "            )\n",
    "            session_manager.add_turn(session_id, conversation_turn)\n",
    "            print(f\"üíæ Saved conversation turn to session {session_id}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚úÖ Final Response: {response['response']}\")\n",
    "        if response['is_followup']:\n",
    "            print(f\"üîó Follow-up detected - Enhanced query: {response['enhanced_query']}\")\n",
    "        \n",
    "        # Show workflow execution details\n",
    "        workflow_plan = response['execution_details'].get('workflow_plan', {})\n",
    "        if workflow_plan:\n",
    "            print(f\"üéØ Execution Path Used: {workflow_plan.get('execution_path', [])}\")\n",
    "            print(f\"üß† Reasoning: {workflow_plan.get('reasoning', 'N/A')}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        error_response = {\n",
    "            \"query\": user_query,\n",
    "            \"response\": \"I encountered an error while processing your request.\",\n",
    "            \"session_id\": session_id,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"execution_details\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"‚ùå Error processing query: {e}\")\n",
    "        return error_response\n",
    "\n",
    "\n",
    "def clear_session(session_id: str):\n",
    "    \"\"\"\n",
    "    üóëÔ∏è Clear conversation history for a session\n",
    "    \"\"\"\n",
    "    session_manager.clear_session(session_id)\n",
    "    print(f\"üóëÔ∏è Cleared session: {session_id}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Main chatbot function ready with dynamic workflow support!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test the Dynamic Workflow System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test different query types to see dynamic workflow in action\n",
    "\n",
    "def test_dynamic_workflow():\n",
    "    \"\"\"Test different query types to demonstrate dynamic workflow capabilities\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing Dynamic Workflow System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_queries = [\n",
    "        {\n",
    "            \"query\": \"Hello, how are you?\",\n",
    "            \"expected_path\": \"Should skip most tools (greeting/irrelevant)\",\n",
    "            \"session\": \"test_greeting\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What was the transaction volume last Friday?\",\n",
    "            \"expected_path\": \"Should use full data analysis path\",\n",
    "            \"session\": \"test_simple\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How does that compare to the previous week?\",\n",
    "            \"expected_path\": \"Should use context enhancement for follow-up\",\n",
    "            \"session\": \"test_simple\"  # Same session for follow-up\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüî¨ Test {i}: {test['query']}\")\n",
    "        print(f\"Expected: {test['expected_path']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            result = process_query(test['query'], test['session'])\n",
    "            \n",
    "            print(f\"‚úÖ Success: {result['success']}\")\n",
    "            print(f\"üìù Response: {result['response'][:100]}...\")\n",
    "            \n",
    "            if 'execution_details' in result:\n",
    "                details = result['execution_details']\n",
    "                print(f\"üéØ Intent: {details.get('intent', 'N/A')}\")\n",
    "                print(f\"üîç Relevant: {details.get('is_relevant', 'N/A')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Test failed: {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Uncomment the line below to run the tests\n",
    "# test_dynamic_workflow()\n",
    "\n",
    "print(\"üéØ Dynamic workflow test function ready!\")\n",
    "print(\"Call test_dynamic_workflow() to see the adaptive routing in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interactive Demo with Dynamic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_demo():\n",
    "    \"\"\"\n",
    "    üéÆ Interactive demo function for testing the dynamic workflow chatbot\n",
    "    \"\"\"\n",
    "    print(\"üéâ Welcome to the Dynamic LangGraph Chatbot with CFG GenAI!\")\n",
    "    print(\"üß† NEW: Intelligent workflow orchestration that adapts to your queries\")\n",
    "    print(\"‚ú® Features: Context-aware routing, smart optimizations, adaptive execution\")\n",
    "    print(\"Type 'quit' to exit, 'clear' to clear session history\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    session_id = \"interactive_session\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüí¨ You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'clear':\n",
    "                clear_session(session_id)\n",
    "                continue\n",
    "            elif not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Process the query with dynamic workflow\n",
    "            result = process_query(user_input, session_id)\n",
    "            \n",
    "            # Display the response\n",
    "            print(f\"\\nü§ñ Bot: {result['response']}\")\n",
    "            \n",
    "            # Show workflow insights\n",
    "            if result.get('execution_details', {}).get('workflow_plan'):\n",
    "                plan = result['execution_details']['workflow_plan']\n",
    "                print(f\"\\nüîç Workflow Insights:\")\n",
    "                print(f\"   Path: {plan.get('execution_path', [])}\")\n",
    "                print(f\"   Type: {plan.get('query_type', 'unknown')}\")\n",
    "                print(f\"   Complexity: {plan.get('complexity', 'unknown')}\")\n",
    "            \n",
    "            # Show additional details if there was an error\n",
    "            if not result['success'] and result['error']:\n",
    "                print(f\"‚ùå Error: {result['error']}\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "# Uncomment the line below to start the interactive demo\n",
    "# interactive_demo()\n",
    "\n",
    "print(\"üéØ Interactive demo function ready!\")\n",
    "print(\"Call interactive_demo() to start chatting with the dynamic workflow bot.\")\n",
    "print(\"\\nüöÄ The bot will show you exactly how it adapts its execution path for each query!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Dynamic Workflow Summary\n",
    "\n",
    "### üéØ **What Makes This Workflow Dynamic?**\n",
    "\n",
    "1. **üß† Intelligent Orchestrator**: Analyzes each query and creates a custom execution plan\n",
    "2. **üîÄ Adaptive Routing**: Different paths for different query types (greetings, data queries, follow-ups)\n",
    "3. **‚ö° Performance Optimization**: Skips unnecessary tools for faster responses\n",
    "4. **üîó Context Awareness**: Automatically handles follow-up questions with conversation memory\n",
    "5. **üõ°Ô∏è Robust Fallbacks**: Graceful error handling with structured data formatting\n",
    "\n",
    "### üìä **Example Execution Paths:**\n",
    "\n",
    "**Simple Data Query:**\n",
    "```\n",
    "\"What was sales volume yesterday?\"\n",
    "‚Üí context_enhancer ‚Üí orchestrator ‚Üí intent_classifier ‚Üí nl_to_sql ‚Üí sql_executor ‚Üí response_formatter\n",
    "```\n",
    "\n",
    "**Follow-up Question:**\n",
    "```\n",
    "\"How does that compare to last month?\"\n",
    "‚Üí context_enhancer ‚Üí orchestrator ‚Üí nl_to_sql ‚Üí sql_executor ‚Üí response_formatter\n",
    "(Skips intent_classifier - obviously relevant)\n",
    "```\n",
    "\n",
    "**Greeting/Irrelevant:**\n",
    "```\n",
    "\"Hello, how are you?\"\n",
    "‚Üí context_enhancer ‚Üí orchestrator ‚Üí intent_classifier ‚Üí END\n",
    "(Skips all data processing tools)\n",
    "```\n",
    "\n",
    "### üîß **Configuration Options:**\n",
    "\n",
    "- `MAX_FULL_RESULTS`: Show all results up to this limit (default: 50)\n",
    "- `MAX_SAMPLE_RESULTS`: Show this many when truncating (default: 20)\n",
    "- Orchestrator prompts can be tuned for different routing strategies\n",
    "\n",
    "### üöÄ **Ready to Use!**\n",
    "\n",
    "The dynamic workflow system is now fully operational and will intelligently adapt its execution based on your queries. Try different types of questions to see the adaptive routing in action!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}