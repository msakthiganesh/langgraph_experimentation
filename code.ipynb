{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from langchain_core.callbacks import (\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.messages.ai import UsageMetadata\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from pydantic import Field, PrivateAttr, BaseModel\n",
    "from cfgenerator import CFGeneratorCM\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np\n",
    "import snowflake.connector\n",
    "import json\n",
    "# Fixed: removed duplicate 'import' keyword\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "# Note: AIMessage and ToolMessage already imported above, removing duplicates\n",
    "# from langchain_core.messages import AIMessage, ToolMessage\n",
    "# Note: Optional and Any already imported above, removing duplicates\n",
    "from typing import Sequence\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import create_react_agent, ToolNode, tool_node\n",
    "# from langgraph.prebuilt import create_react_agent_state\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from typing import TypedDict, List\n",
    "# Note: BaseMessage already imported above, removing duplicate\n",
    "# from langchain_core.messages import BaseMessage\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592da391",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_ID = \"md0005_openai_gpt4o\"  # \"md0002_openai_gpt3516k\"\n",
    "# gdk = CFGenAIGDK(\"uc0027_abct_rag_cl\", \"rag_cl\", \"ChatBot to assist Business\")\n",
    "USE_STATIC = True  # Use the static workflow generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48478c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIPCall:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model_id: str = \"md0005_openai_gpt4o\",\n",
    "        def_max_tokens: int = 200,\n",
    "        def_temperature: float = 0.4,\n",
    "        expt_name: str = \"uc0027_abct_rag_cl\",\n",
    "        expt_id: str = \"rag_cl\",\n",
    "        expt_desc: str = \"ChatBot to assist Business\"\n",
    "    ):\n",
    "        self.MODEL_ID = llm_model_id\n",
    "        self.MAX_TOKENS = def_max_tokens\n",
    "        self.TEMP = def_temperature\n",
    "        self.EXP_NAME = expt_name\n",
    "        self.EXP_ID = expt_id\n",
    "        self.EXP_DESC = expt_desc\n",
    "\n",
    "        # Create an instance of the gdk!\n",
    "        self.gdk = CFGeneratorCM(self.EXP_NAME, self.EXP_ID, self.EXP_DESC)\n",
    "\n",
    "    def invoke_llm_response(self, prompt: Optional[str], **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Invokes the gdk response from a model, given a prompt and set of parameters passed through\n",
    "        kwargs\n",
    "        \"\"\"\n",
    "\n",
    "        placeholders = {k: v for k, v in kwargs.items() if (\n",
    "            k != 'max_tokens' or k != 'temperature' or k != 'user_query')}\n",
    "\n",
    "        unpacked_vals = {k: v for k, v in kwargs.items() if (\n",
    "            k == 'max_tokens' or k == 'temperature' or k == 'user_query')}\n",
    "\n",
    "        max_tokens = kwargs.get('max_tokens', self.MAX_TOKENS)\n",
    "        temperature = kwargs.get('temperature', self.TEMP)\n",
    "        # we account for there being no value for the user_query attribute during this call\n",
    "        user_query = kwargs.get('user_query', \"\")\n",
    "        prompt = prompt.format(**placeholders)\n",
    "\n",
    "        if user_query != \"\":\n",
    "            prompt_template = {\n",
    "                \"prompt_template\": [\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_query}\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            prompt_template = {\n",
    "                \"prompt_template\": [{\"role\": \"system\", \"content\": prompt}]\n",
    "            }\n",
    "\n",
    "        response = self.gdk.invoke_llmgateway(\n",
    "            prompt_template,\n",
    "            {\"max_tokens\": max_tokens, \"temperature\": temperature},\n",
    "            self.MODEL_ID\n",
    "        )\n",
    "\n",
    "        content_filter_results = response[\"genResponse\"][\"choices\"][0][\"content_filter_results\"]\n",
    "        ai_message = response[\"genResponse\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        return ai_message\n",
    "\n",
    "\n",
    "print(\"Done defining gaip services class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6666201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    messages: List[BaseMessage]\n",
    "    use_static: Optional[str]\n",
    "    workflow: Optional[Any] = None\n",
    "    tool_name: Optional[str]\n",
    "    tool_input: Optional[str]\n",
    "    tool_number: Optional[int] = 0\n",
    "    done: Optional[bool] = False\n",
    "\n",
    "\n",
    "print(\"Done defining the agent state class TypedDict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentOrchestrator(BaseChatModel):\n",
    "    _LLM_MODEL_ID: str = PrivateAttr()\n",
    "    _DEF_MAX_TOKENS: str = PrivateAttr()\n",
    "    _DEF_TEMPERATURE: str = PrivateAttr()\n",
    "    _expt_name: str = PrivateAttr()\n",
    "    _expt_id: str = PrivateAttr()\n",
    "    _expt_desc: str = PrivateAttr()\n",
    "    USE_STATIC: bool = PrivateAttr()\n",
    "    _gaip = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model_id: str = \"md0005_openai_gpt4o\",\n",
    "        def_max_tokens: int = 200,\n",
    "        def_temperature: float = 0.4,\n",
    "        expt_name: str = \"uc0027_abct_rag_cl\",\n",
    "        expt_id: str = \"rag_cl\",\n",
    "        expt_desc: str = \"ChatBot to assist Business\",\n",
    "        use_static: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Makes sure all keyword params are inherited from the parent BaseChatModel class\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self._LLM_MODEL_ID = llm_model_id\n",
    "        self._DEF_MAX_TOKENS = def_max_tokens\n",
    "        self._DEF_TEMPERATURE = def_temperature\n",
    "        self._expt_name = expt_name\n",
    "        self._expt_id = expt_id\n",
    "        self._expt_desc = expt_desc\n",
    "\n",
    "        # Toggle between static and dynamic workflows\n",
    "        # use_static: True generates a workflow with tool queue at the beginning and follows the steps until the end\n",
    "        # use_static: False Dynamically decides what tool to use next at every step depending on the response of the previous tool and other inputs\n",
    "        self.USE_STATIC = use_static\n",
    "\n",
    "        # Create an instance of the GAIPCall class\n",
    "        self._gaip = GAIPCall(\n",
    "            self._LLM_MODEL_ID,\n",
    "            self._DEF_MAX_TOKENS,\n",
    "            self._DEF_TEMPERATURE,\n",
    "            self._expt_name,\n",
    "            self._expt_id,\n",
    "            self._expt_desc\n",
    "        )\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], **kwargs) -> ChatResult:\n",
    "        \"\"\"\n",
    "        DYNAMIC GENERATE: the model chooses the tool and the input to be given to the tool to be invoked\n",
    "        Invokes the gdk's function to return a response and adds the response to AIMessage\n",
    "        \"\"\"\n",
    "\n",
    "        if self.USE_STATIC:\n",
    "\n",
    "            # Fetch only the latest human message for intent identification\n",
    "\n",
    "            # CALL THE STATIC GENERATE FUNCTION\n",
    "            response = self.static_generate(messages)\n",
    "\n",
    "            return response\n",
    "\n",
    "    def id_intent_workflow(self, messages: List[BaseMessage]) -> str:\n",
    "        \"\"\"\n",
    "        Identify the intent of the user message to choose the most appropriate workflow to use\n",
    "        \"\"\"\n",
    "\n",
    "        # prompt to identify the intent\n",
    "        intent_followup_prompt = \"\"\"\n",
    "        Your task is to consider the user's message and identify two things based on it:\n",
    "        1. The intent behind the message      \n",
    "        2. If the question is a followup to a previous one\n",
    "        \n",
    "        Task 1: To identify the intent behind the messages, follow these guidelines:\n",
    "        - PCT Query: If the user's message is about payment details, payment trends, transaction sums, \n",
    "          averages, mins and maxs, AML checks and fraud checks, the intent is PCT Query.\n",
    "        - Weather: If the user's message is about the weather of any place, the intent is simply Weather\n",
    "        - Miscellaneous: If the user's message is not about any of the above, the intent is Miscellaneous\n",
    "        \n",
    "        Task 2: To identify if the user message is a followup to a previous user message, use these guidelines:\n",
    "        - Start by considering the previous few pairs of user messages and their AI responses\n",
    "        \n",
    "        Respond in this format and with no additional text or markdown if you identify that this message is a \n",
    "        followup to a previous user message\n",
    "        {{\n",
    "            intent: <identified intent>,\n",
    "            followup_to: <prev user message this message is a followup to>,\n",
    "            prev_ai_msg: <ai response to prev user message this message is a followup to>,\n",
    "        }}\n",
    "        \n",
    "        Response in this format and with no additional text or markdown if you identify that this message is \n",
    "        not a followup to any previous user message\n",
    "        {{\n",
    "            intent: <identified intent>,\n",
    "            followup_to: None,\n",
    "            prev_ai_msg: None\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        intent_prompt = \"\"\"\n",
    "        Your task is to consider the user's message and identify the intent behind the message.\n",
    "        \n",
    "        To identify the intent behind the messages, follow these guidelines:\n",
    "        - PCT Query: If the user's message is about payment details, payment trends, transaction sums, \n",
    "          averages, mins and maxs, AML checks and fraud checks, the intent is PCT Query.\n",
    "        - Weather: If the user's message is about the weather of any place, the intent is simply Weather\n",
    "        - Miscellaneous: If the user's message is not about any of the above, the intent is Miscellaneous\n",
    "        \n",
    "        Respond in this format and with no additional text or markdown\n",
    "        {{\n",
    "            \"intent\": <identified intent>\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        human_messages = [\n",
    "            m.content for m in messages if isinstance(m, HumanMessage)]\n",
    "        last_user_message = human_messages[-1]\n",
    "        prev_user_message = human_messages[-2] if len(\n",
    "            human_messages) > 1 else \"\"\n",
    "\n",
    "        params = {\"max_tokens\": 50, \"temperature\": 0.1,\n",
    "                  \"user_query\": last_user_message}\n",
    "        # Identify the intent\n",
    "        ai_response_intent = self._gaip.invoke_llm_response(\n",
    "            intent_prompt,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            ai_response_json = json.loads(ai_response_intent)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\n",
    "                \"Something went wrong when trying to convert the ai response string into a JSON!\")\n",
    "\n",
    "        print(\"ai_response_intent after self.gaip.invoke_llm_response():\")\n",
    "        # If the intent is PCT Query, return the prompt to generate the workflow\n",
    "        if ai_response_json[\"intent\"].lower() == \"pct query\":\n",
    "            workflow_prompt = \"\"\"\n",
    "            Your task is to create a workflow JSON provided to you as input and generate a workflow JSON.\n",
    "            The workflow must be structured like in the provided format. Your response must only contain the valid JSON and nothing \n",
    "            else, no additional text or markdown or anything.\n",
    "            \n",
    "            EXAMPLE JSON FORMAT:           \n",
    "            {{\n",
    "                \"output\": {{\n",
    "                    \"intent\": \"PCT Query\",\n",
    "                    \"workflow\": [\n",
    "                        {{\n",
    "                            \"step\": 1,\n",
    "                            \"type\": \"llm\",\n",
    "                            \"function_name\": \"nl_to_sql\",\n",
    "                            \"prompt\": \"Convert the following natural language query into a SQL statement to be executed on a Snowflake DB: '<USER QUERY>'\",\n",
    "                            \"payload\": {{\n",
    "                                \"user_query\": \"<USER QUERY>\"\n",
    "                            }}\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"step\": 2,\n",
    "                            \"type\": \"database\",\n",
    "                            \"function_name\": \"snowflake_executor\",\n",
    "                            \"payload\": {{\n",
    "                                \"sql\": \"<sql output>\"\n",
    "                            }}\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"step\": 3,\n",
    "                            \"type\": \"llm\",\n",
    "                            \"function_name\": \"response_formatter\",\n",
    "                            \"prompt\": \"Use the data from the Snowflake sql query response provided to synthesize a natural language response to the user's original query: '<USER QUERY>'\",\n",
    "                            \"payload\": {{\n",
    "                                \"raw_data\": \"snowflake_executor_output\",\n",
    "                                \"user_query\": \"<USER QUERY>\"\n",
    "                            }}\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            }}\n",
    "            \n",
    "            While generating your valid JSON workflow, follow these rules:\n",
    "            - Take care to replace all placeholders within the workflow with the actual value expected in those spots.\n",
    "            - As long as the user's query is related to payment details, transactions, fraud checks, AML checks, the intent must always be 'PCT Query'\n",
    "            - The entire workflow must always contain 3 steps:\n",
    "            - These are the three steps that must be contained within the list against the 'workflow' attribute:\n",
    "            - Step 1:\n",
    "                - The \"step\" must be \"1\"\n",
    "                - The \"type\" must be \"llm\"\n",
    "                - The \"function_name\" must be \"nl_to_sql\"\n",
    "                - The \"prompt\" must be with \"Convert the following natural language query into a SQL statement to be executed on a Snowflake DB: <USER QUERY>\" with the placeholder replaced by the actual user query\n",
    "                - Finally, the \"payload\" must be a dictionary with the attribute \"user_query\" containing against it the actual user's query instead of \"<USER QUERY>\"\n",
    "            - Step 2:\n",
    "                - The \"step\" must be \"2\"\n",
    "                - The \"function_name\" must be \"snowflake_executor\"\n",
    "                - The \"type\" must be \"database\"\n",
    "                - Finally, the \"payload\" must be a dictionary with the attribute \"sql\" containing the string \"step_1_output.sql\"\n",
    "            - Step 3:\n",
    "                - The \"step\" must be \"3\"\n",
    "                - The \"type\" must be \"llm\"\n",
    "                - The \"function_name\" must be \"response_formatter\"\n",
    "                - The \"prompt\" must be \"Use the data from the Snowflake sql query response provided to synthesize a natural language response to the user's original query: '<USER QUERY>'\"\n",
    "                - Finally, the \"payload\" must have the \"raw_data\" containing the string \"step_2_output.data\" and \"user_query\" containing the actual user query instead of \"<USER QUERY>\"\n",
    "            \"\"\"\n",
    "\n",
    "        elif ai_response_json[\"intent\"].lower() == \"weather\":\n",
    "            workflow_prompt = \"\"\"\n",
    "            Your task is to identify the name of a city in the user's query and respond with a JSON like in example provided below and nothing else.\n",
    "            Make sure to substitute all the placeholders indicated by \"< >\" with the appropriate values instead.\n",
    "            \n",
    "            {{\n",
    "                \"output\": {{\n",
    "                    \"intent\": \"Weather\",\n",
    "                    \"workflow\": [\n",
    "                        {{\n",
    "                            \"step\": 1,\n",
    "                            \"type\": \"llm\",\n",
    "                            \"function_name\": \"get_weather\",\n",
    "                            \"prompt\": \"Understand the user's question, identify the name of the city in the query and use it to return this descriptive string in response: 'The weather in <identified city> is partly sunny with a high of 80°F.'\",\n",
    "                            \"payload\": {{\n",
    "                                \"user_query\": \"<USER QUERY>\"\n",
    "                            }}\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "        else:\n",
    "            workflow_prompt = \"\"\"\n",
    "            Your task is to construct and return this JSON, substituting the placeholder <USER QUERY> with the actual user query.              \n",
    "            {{\n",
    "                \"output\": {{\n",
    "                    \"intent\": \"Miscellaneous\",\n",
    "                    \"workflow\": [\n",
    "                        {{\n",
    "                            \"step\": 1,\n",
    "                            \"type\": \"llm\",\n",
    "                            \"function_name\": \"general_response\",\n",
    "                            \"prompt\": \"Understand the user's question and reply with at most 3 sentences. If you do not understand the question or do not have sufficient information about it, respond with 'I am sorry, I cannot help you with that.'\",\n",
    "                            \"payload\": {{\n",
    "                                \"user_query\": \"<USER QUERY>\"\n",
    "                            }}\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "        # Generate the workflow using the prompt\n",
    "        params = {\n",
    "            \"max_tokens\": 1200,\n",
    "            \"temperature\": 0.3,\n",
    "            \"user_query\": last_user_message\n",
    "        }\n",
    "        ai_response = self._gaip.invoke_llm_response(\n",
    "            workflow_prompt,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        return ai_response  # for which should contain a valid JSON\n",
    "\n",
    "    def static_generate(self, messages: List[BaseMessage], **kwargs) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Use the static thing generator does not return anything, it just loads to be invoked for a given input at\n",
    "        once in the form of a loop - structure provided in example\"\"\"\n",
    "\n",
    "        workflow_string = self.id_intent_workflow(\n",
    "            messages)  # this returns a string for now\n",
    "\n",
    "        # Check if the response is a valid JSON before passing it on...\n",
    "        try:\n",
    "            workflow_json = json.loads(workflow_string)\n",
    "            except_json_valid_workflow_generated = True\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\n",
    "                \"JSONDecodeError when trying to validate the AI generated JSON workflow: {e}\")\n",
    "            # CONTINUE: Does it make sense to attempt again? If yes, what is the exit strategy?\n",
    "            # For now:\n",
    "            except_json_valid_workflow_generated = False\n",
    "\n",
    "        message = AIMessage(\n",
    "            content=\"I apologize, but I encountered an error while processing your request.\",\n",
    "            response_metadata={\n",
    "                \"source_type\": \"gaip_model\",\n",
    "                \"model_name\": \"static_generate\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        generations = [generation]\n",
    "\n",
    "        ai_msg = ChatResult(generations=generations)\n",
    "        return ai_msg\n",
    "\n",
    "    def bind_tools(self, tools: Sequence[Any]) -> \"AgentOrchestrator\":\n",
    "        \"\"\"Creates a new instance of the AgentOrchestrator class with the bound_tools attribute and returns it\"\"\"\n",
    "        print(\"** bind_tools called! List of tools passed:\",\n",
    "              [t.name for t in tools])\n",
    "        new_instance = self.__class__(\n",
    "            llm_model_id=self._LLM_MODEL_ID,\n",
    "            def_max_tokens=self._DEF_MAX_TOKENS,\n",
    "            def_temperature=self._DEF_TEMPERATURE,\n",
    "            expt_name=self._expt_name,\n",
    "            expt_id=self._expt_id,\n",
    "            expt_desc=self._expt_desc,\n",
    "            use_static=self.USE_STATIC\n",
    "        )\n",
    "        object.__setattr__(new_instance, \"_LLM_MODEL_ID\", self._LLM_MODEL_ID)\n",
    "        object.__setattr__(new_instance, \"_DEF_MAX_TOKENS\",\n",
    "                           self._DEF_MAX_TOKENS)\n",
    "        object.__setattr__(new_instance, \"_DEF_TEMPERATURE\",\n",
    "                           self._DEF_TEMPERATURE)\n",
    "        object.__setattr__(new_instance, \"_expt_name\", self._expt_name)\n",
    "        object.__setattr__(new_instance, \"_expt_id\", self._expt_id)\n",
    "        object.__setattr__(new_instance, \"_expt_desc\", self._expt_desc)\n",
    "        object.__setattr__(new_instance, \"USE_STATIC\", self.USE_STATIC)\n",
    "        object.__setattr__(new_instance, \"_gaip\", self._gaip)\n",
    "        object.__setattr__(new_instance, \"bound_tools\", tools)\n",
    "        return new_instance\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._param_name = \"uc0027_abct_rag_cl\"\n",
    "        self._param_id = \"rag_cl\"\n",
    "        self._param_desc = \"ChatBot to assist Business\"\n",
    "        self._gdk = CFGeneratorCM(\n",
    "            self._param_name, self._param_id, self._param_desc)\n",
    "        self._hyperparams = {\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        self._model_id = llm_model_id\n",
    "\n",
    "        return f\"Citizens CFGGenAIGDK:{self._LLM_MODEL_ID}\"\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9122302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tool #1\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"A tool that returns the weather of a city. It accepts the name of a city as input and returns a\n",
    "    string describing the weather in that city as output.\n",
    "    \"\"\"\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(\"\\\\n🌤️ Executing Get Weather Tool\")\n",
    "\n",
    "    return f\"The weather in Dallas is sunny with a high of 85°F.\"\n",
    "\n",
    "\n",
    "class getWeatherToolInput(BaseModel):\n",
    "    city: str\n",
    "\n",
    "\n",
    "get_weather_tool = Tool.from_function(\n",
    "    func=get_weather,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Returns the weather for a given city.\",\n",
    "    args_schema=getWeatherToolInput\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file_path = 'Docs/payment_data_dictionary_1.xlsx'\n",
    "    df1 = pd.read_excel(file_path, sheet_name=\"payment_details\")\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(df1)\n",
    "    df2 = pd.read_excel(file_path, sheet_name=\"payment_metrics\")\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(df2)\n",
    "    df3 = pd.read_excel(file_path, sheet_name=\"payment_kpi_daily_summary\")\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(df3)\n",
    "    schema_data = f\"\"\"Schema and column descriptions of PAYMENT_DETAILS table:\\\\n\n",
    "    {df1}\n",
    "    \n",
    "    Schema and column descriptions of PAYMENT_METRICS table:\n",
    "    {df2}\n",
    "    \n",
    "    Schema and column descriptions of PAYMENT_KPI_DAILY_SUMMARY table:\n",
    "    {df3}\n",
    "    \"\"\"\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"An Exception occurred when trying to read the payment_data_dictionary file: {e}\")\n",
    "\n",
    "schema_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb33fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def nl_to_sql(tool_input) -> str:\n",
    "    \"\"\"\n",
    "    Converts a natural language question into a Snowflake-compatible SQL query. Accepts the user's question as input, and returns the generated SQL query as output.\n",
    "    Previous exchange: (hist_context)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    # user_query = input_payload[\"user_query\"]\n",
    "    print(\"\\\\n🔍 Executing NL to SQL Tool\")\n",
    "    print(f\"\\\\nAccepted TOOL INPUT: string <User's query>\")\n",
    "    print(f\"TOOL INPUT RECEIVED: {tool_input}\\\\nTYPE: {type(tool_input)}\")\n",
    "\n",
    "    # No need to parse this input..\n",
    "    user_query = tool_input\n",
    "\n",
    "    # today\n",
    "    today_date = datetime.now()\n",
    "    date = today_date.strftime(\"%Y-%m-%d\")\n",
    "    day = today_date.strftime(\"%A\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a Snowflake SQL expert. Convert the following natural language question into a valid SQL query.\n",
    "    Return only the generated SQL query in your response, no additional text or explanations. Make sure to\n",
    "    follow the rules while generating the SQL query, and use the schema for context - this will help you\n",
    "    generate the most relevant queries for the user's input.\n",
    "    \n",
    "    Rules:\n",
    "    - ONLY USE SELECT QUERIES\n",
    "    - DO NOT USE UPDATE, INSERT, DELETE, JOIN in your queries\n",
    "    - Use proper Snowflake SQL syntax\n",
    "    - Use table and column names exactly as shown in the schema\n",
    "    - When specific dates or days are mentioned in the query, pay special attention to them and format the\n",
    "      query with the right date or range of dates by calculating the correct values to go in the query based\n",
    "      on today's date and day provided to you. For example, if the user asks for information about the previous\n",
    "      Tuesday, consider the date and day it is today and count back to the previous Tuesday to deduce the correct\n",
    "      date to be used in the query.\n",
    "    - Add LIMIT 20 for queries that might return many rows or pick the latest 20 rows\n",
    "    - Use uppercase for SQL keywords like SELECT, FROM, WHERE, etc.\n",
    "    - Table names should be unquoted if they don't contain special characters\n",
    "    - Don't use limit for aggregations LIMIT\n",
    "    - Return ONLY the SQL query, no explanations or markdown\n",
    "    \n",
    "    Tables' Schema:\n",
    "    {schema_data}\n",
    "    \n",
    "    Today's date for reference:\n",
    "    {date}\n",
    "    \n",
    "    Today's day for reference:\n",
    "    {day}\n",
    "    \"\"\"\n",
    "\n",
    "    hyper_params = {\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    gdk = CFGGenAIGDK(\"uc0027_abct_rag_cl\", \"rag_cl\",\n",
    "                      \"ChatBot to assist Business\")\n",
    "    prompt_template = {\n",
    "        \"prompt_template\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_query\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = gdk.invoke_llmgateway(\n",
    "        prompt_template,\n",
    "        hyper_params,\n",
    "        LLM_MODEL_ID\n",
    "    )\n",
    "\n",
    "    generated_SQL = response[\"genResponse\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return generated_SQL\n",
    "\n",
    "\n",
    "class nl2SQLToolInput(BaseModel):\n",
    "    user_query: str\n",
    "\n",
    "\n",
    "nl_to_sql_tool = Tool.from_function(\n",
    "    func=nl_to_sql,\n",
    "    name=\"nl_to_sql\",\n",
    "    description=\"Returns a Snowflake compatible SQL query for the PCI Database.\",\n",
    "    args_schema=nl2SQLToolInput\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tool #3 - SQL execution tool\n",
    "def create_snowflake_connection():\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            user='SNOWFLAKE_PCI_PE_RO_P2',\n",
    "            password='MhgfrwqcvfdR6mgte',\n",
    "            account='citizensbank-p2.privatelink',\n",
    "            warehouse='BCT_WH',\n",
    "            database='BUSINESS_CONTROL_TOWER',\n",
    "            schema='PAYMENT_CONTROL_TOWER',\n",
    "            role='SNOWFLAKE_ROLE_PCI_PE_RO_P2'\n",
    "        )\n",
    "        print(\"Connected to Snowflake\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Snowflake connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def execute_snowflake_query(query, conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Query Execution Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@tool\n",
    "def execute_snowflake_sql(tool_input) -> str:\n",
    "    \"\"\"A tool that accepts a SQL query as input, executes it on a Snowflake DB through\n",
    "    an API connection and returns the response from the query as output\"\"\"\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(\"\\\\n🗄️ Executing Snowflake SQL Tool\")\n",
    "    # sql_query = input_payload[\"sql_query\"]\n",
    "    print(f\"\\\\nExpected TOOL INPUT: string <SQL query>\")\n",
    "    print(f\"TOOL INPUT RECEIVED: {tool_input}\\\\nTYPE: {type(tool_input)}\")\n",
    "\n",
    "    conn = create_snowflake_connection()\n",
    "    if not (tool_input.startswith('SELECT') or tool_input.startswith('WITH')):\n",
    "        return \"Unable to execute sql_query. No response returned\"\n",
    "    result = execute_snowflake_query(tool_input, conn)\n",
    "    print(f\"\\\\nResult of executed SQL query:\")\n",
    "    print(result)\n",
    "    conn.close()\n",
    "    return str(result)  # wrap the result into a string\n",
    "\n",
    "\n",
    "class sqlExecutionToolInput(BaseModel):\n",
    "    sql_query: str\n",
    "\n",
    "\n",
    "sql_execution_tool = Tool.from_function(\n",
    "    func=execute_snowflake_sql,\n",
    "    name=\"execute_snowflake_sql\",\n",
    "    description=\"Executes a sql query on Snowflake and returns the results.\",\n",
    "    args_schema=sqlExecutionToolInput\n",
    ")\n",
    "\n",
    "print(\"Done defining the SQL execution tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tool #4 - response synthesizer\n",
    "\n",
    "@tool\n",
    "def synthesize_response(tool_input: dict) -> str:\n",
    "    \"\"\"A tool that accepts context containing the user's natural language query, the SQL query generated for it,\n",
    "    as well as the results of the executed SQL query from the Snowflake database, and synthesizes a final human\n",
    "    comprehensible response as output to the user's question.\"\"\"\n",
    "\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(\"\\\\n🧠 Executing Synthesize Response Tool\")\n",
    "\n",
    "    raw_data = tool_input[\"raw_data\"]\n",
    "    user_query = tool_input[\"user_query\"]\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful AI assistant whose task is to consider the user's query, \\\\\n",
    "    and the response of its corresponding SQL query execution on a Snowflake database \\\\\n",
    "    to synthesize a final response which is human comprehensible and easy to read.\n",
    "    \n",
    "    When responding, keep in mind that you are answering the user's query; \\\\\n",
    "    and the results of SQL query execution contain all the information you will require to \\\\\n",
    "    answer the user's query in a meaningful way.\n",
    "    \n",
    "    Results of SQL query execution as context for your final response:\n",
    "    {raw_data}\n",
    "    \n",
    "    If the SQL execution result has returned multiple rows of structured data in the form of tuples \\\\\n",
    "    within a list, format your response to look like this by separating out individual values: \\\\\n",
    "    \n",
    "    | column 1      | column 2      | column 3      | column 4      |\n",
    "    |---------------|---------------|---------------|---------------|\n",
    "    | data          | data          | data          | data          |\n",
    "    | data          | data          | data          | data          |\n",
    "    \n",
    "    If the result of the sql query execution is a single row or a count value, just respond with a single \\\\\n",
    "    descriptive sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    hyper_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.35\n",
    "    }\n",
    "    prompt = {\n",
    "        \"prompt_template\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_query\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    model_id = LLM_MODEL_ID = \"md0002_openai_gpt3516k\"  # \"md0005_openai_gpt4omini\"\n",
    "\n",
    "    gdk = CFGeneratorCM(\"uc0027_abct_rag_cl\", \"rag_cl\",\n",
    "                        \"ChatBot to assist Business\")\n",
    "    response = gdk.invoke_llmgateway(\n",
    "        prompt,\n",
    "        hyper_params,\n",
    "        model_id\n",
    "    )\n",
    "\n",
    "    return response[\"genResponse\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # class synthesizeResponseToolInput(BaseModel):\n",
    "    # tool_input: Optional[Any]\n",
    "\n",
    "\n",
    "synthesize_response_tool = Tool.from_function(\n",
    "    func=synthesize_response,\n",
    "    name=\"synthesize_response\",\n",
    "    description=\"Accepts the user's query, and the results of the corresponding SQL query execution, and returns a final synthesized response to the user's original query.\",\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the one globally used instance of the class with the bind_tools in place\n",
    "\n",
    "# CHANGE use_static = False FOR DYNAMIC TOOL CALLING\n",
    "model = AgentOrchestrator(\n",
    "    llm_model_id=LLM_MODEL_ID, def_max_tokens=200, def_temperature=0.4, expt_name=\"uc0027_abct_rag_cl\",\n",
    "    expt_id=\"rag_cl\", expt_desc=\"ChatBot to assist Business\", use_static=True\n",
    ")\n",
    "\n",
    "# llm_model_id=LLM_MODEL_ID, use_static=False, max_tokens=300)\n",
    "model = model.bind_tools([\n",
    "    nl_to_sql_tool,\n",
    "    get_weather_tool,\n",
    "    sql_execution_tool,\n",
    "    synthesize_response_tool\n",
    "])\n",
    "\n",
    "print(\"Done initializing a model instance and binding all tools to it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end(state: AgentState) -> AgentState:\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(\"\\\\n Reached end of graph! Exiting.\")\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"Done defining end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be035b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_done(state: AgentState) -> str:\n",
    "    return \"end\" if state.get(\"done\") else \"interrupt\"\n",
    "\n",
    "\n",
    "print(\"Done defining the check done message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_v6(state: AgentState) -> AgentState:\n",
    "\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(\"\\\\n Interpreting...\\\\n\")\n",
    "\n",
    "    if state.get(\"use_static\") is True and state.get(\"workflow\", None) == None:\n",
    "\n",
    "        # IF STATIC DO self.static_generate\n",
    "        # response = self.static_generate(state[\"messages\"])\n",
    "        # ONLY DONE ONCE!!\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "\n",
    "        workflow_string = response.content  # JSON wrapped by str\n",
    "\n",
    "        print(\"\\\\nWORKFLOW GENERATED:\\\\n\")\n",
    "        print(workflow_string)\n",
    "\n",
    "        workflow_json = json.loads(workflow_string)\n",
    "\n",
    "        # Update the state with the generated workflow - only once!\n",
    "        state = {\n",
    "            **state,  # preserve existing keys like \"done\"\n",
    "            \"workflow\": workflow_json,\n",
    "            \"messages\": state[\"messages\"] + [response]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        print(\"\\\\nNothing new to do inside interpret this time! Returning state unchanged\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tool_v7(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Function that calls one tool function - depending on the latest populated values of tool_to_use\n",
    "    and tool_input from state\n",
    "    This function should handle tool failures and exceptions!\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\\\n===============================================================================\\\\n\")\n",
    "    print(\"\\\\n🔧 Using Tool...\\\\n\")\n",
    "\n",
    "    # check if \"done\" has been set to True already and return state if so\n",
    "    # print(f\"\\\\nstate: {state}\")\n",
    "    if state.get(\"done\", False):\n",
    "        return state  # Return state, no changes done\n",
    "\n",
    "    if state.get(\"tool_number\", 0) == 0:\n",
    "        state[\"tool_number\"] = 0  # initialize to 0 is if not yet done\n",
    "\n",
    "    # We have a valid state with tool_to_use and tool_input updated by plan_tool_use\n",
    "    tool_name = state.get(\"tool_name\")\n",
    "    tool_input = state.get(\"tool_input\")\n",
    "\n",
    "    if tool_name == \"nl_to_sql\" and tool_input:\n",
    "        # {\"user_query\": \"What is the average number of transactions stopped for AML checks over the first week of September?\"}\n",
    "        print(f\"\\\\n🔍 Calling tool {tool_name} with input: {tool_input}\")\n",
    "        sql_result = nl_to_sql.invoke({\"user_query\": tool_input})\n",
    "        ai_message = AIMessage(\n",
    "            content=sql_result,\n",
    "            response_metadata={\n",
    "                \"source_type\": \"tool_call\",\n",
    "                \"source_name\": \"nl_to_sql\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\\\n🟢 nl_to_sql Tool result: {sql_result}\\\\n\")\n",
    "\n",
    "        # Update the workflow JSON in state before returning it - the next tool should be able to\n",
    "        # grab its input from that workflow..\n",
    "        current_tool_number = state.get(\"tool_number\")\n",
    "\n",
    "        # if the current tool is not the last tool in the workflow, then update the payload of the\n",
    "        # next tool with the output of this tool\n",
    "        state[\"workflow\"][\"output\"][\"workflow\"][current_tool_number +\n",
    "                                                1][\"payload\"].update({\"sql\": sql_result})\n",
    "\n",
    "        # Update tool Number count by one!\n",
    "        state[\"tool_number\"] += 1\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [ai_message]\n",
    "        }\n",
    "\n",
    "    elif tool_name == \"get_weather\" and tool_input:\n",
    "        \"\"\"\n",
    "        structure of the tool_input received by this tool:\n",
    "        tool_input = {\"user_query\": \"user's natural language question>\"}\n",
    "        \"\"\"\n",
    "        # {\"user_query\": \"What is the average number of transactions stopped for AML checks over the first week of September?\"}\n",
    "        user_query_abt_weather = tool_input[\"user_query\"]\n",
    "\n",
    "        print(\n",
    "            f\"\\\\n🌤️ Calling tool {tool_name} with input: {user_query_abt_weather}\")\n",
    "        weather_info = get_weather.invoke({\"user_query_abt_weather\"})\n",
    "        ai_message = AIMessage(\n",
    "            content=weather_info,\n",
    "            response_metadata={\n",
    "                \"source_type\": \"tool_call\",\n",
    "                \"source_name\": \"get_weather\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\\\n🟢 Get Weather TOOL output: {weather_info}\\\\n\")\n",
    "\n",
    "        # Update the workflow JSON in state before returning it - the next tool should be able to\n",
    "        # grab its input from that workflow..\n",
    "        current_tool_number = state.get(\"tool_number\")\n",
    "\n",
    "        # if the current tool is not the last tool in the workflow, then update the payload of the\n",
    "        # next tool with the output of this tool\n",
    "        if current_tool_number < len(state[\"workflow\"][\"output\"][\"workflow\"]) - 1:\n",
    "            state[\"workflow\"][\"output\"][\"workflow\"][current_tool_number +\n",
    "                                                    1][\"payload\"].update({\"weather\": weather_info})\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"done\": True  # Optional: use for conditional edge to END\n",
    "        }\n",
    "\n",
    "    elif tool_name == \"snowflake_executor\" and tool_input:\n",
    "        print(f\"\\n🔧 Calling tool '{tool_name}' with input: {tool_input}\")\n",
    "        query_exec_results = execute_snowflake_sql.invoke(tool_input[\"sql\"])\n",
    "        ai_message = AIMessage(\n",
    "            content=query_exec_results,\n",
    "            response_metadata={\n",
    "                \"source_type\": \"tool_call\",\n",
    "                \"source_name\": \"execute_snowflake_sql\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        current_tool_number = state.get(\"tool_number\")\n",
    "\n",
    "        # If the current tool is not the last tool in the workflow, then update the payload of the\n",
    "        # next tool with the output of this tool\n",
    "        state[\"workflow\"][\"output\"][\"workflow\"][current_tool_number +\n",
    "                                                1][\"payload\"].update({\"raw_data\": query_exec_results})\n",
    "\n",
    "        # Update Tool Number count by one!\n",
    "        state[\"tool_number\"] += 1\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [ai_message]\n",
    "        }\n",
    "\n",
    "    elif tool_name == \"response_formatter\" and tool_input:\n",
    "        print(f\"\\n🔧 Calling tool '{tool_name}' with input: {tool_input}\")\n",
    "        final_response = synthesize_response.invoke({\"tool_input\": tool_input})\n",
    "        ai_message = AIMessage(\n",
    "            content=final_response,\n",
    "            response_metadata={\n",
    "                \"source_type\": \"tool_call\",\n",
    "                \"source_name\": \"synthesize_response\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n✅ Final synthesized response to user:\\n{final_response}\\n\")\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"done\": True  # Optional: use for conditional edge to END\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ Tool not called - missing tool name or input.\")\n",
    "        error_msg = \"Missing tool name or input. Returning.\"\n",
    "        ai_message = AIMessage(\n",
    "            content=error_msg,\n",
    "            response_metadata={\n",
    "                \"source_type\": \"graph_node\",\n",
    "                \"source_name\": \"use_tool\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"done\": True\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Done defining use_tool function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f15816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_tool_use_v4(state: AgentState) -> AgentState:\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    print(\"\\n🧠 Planning Tool Use...\")\n",
    "\n",
    "    # If \"done\" has already been set to true for whatever reason, we continue onwards\n",
    "    if state.get(\"done\", False):\n",
    "        return state\n",
    "\n",
    "    if state[\"use_static\"] is True:\n",
    "\n",
    "        try:\n",
    "            workflow_steps = state[\"workflow\"][\"output\"][\"workflow\"]\n",
    "            # list of only the pipeline names\n",
    "            tool_queue = [step[\"pipeline_name\"] for step in workflow_steps]\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to parse the workflow JSON. Error: {e}\\n\")\n",
    "            return {\n",
    "                **state,\n",
    "                \"tool_name\": next_tool,\n",
    "                \"tool_input\": tool_input\n",
    "            }\n",
    "\n",
    "        # Step 2: Fetch the name of the previous tool called from state, and the first tool if not yet set\n",
    "        prev_tool = state.get(\"tool_name\", None)\n",
    "        print(f\"\\nPrev tool identified: {prev_tool}\")\n",
    "\n",
    "        # By this point we already have a tool queue\n",
    "        if len(tool_queue) > 0:\n",
    "\n",
    "            # Step 3: Find the name of the next tool to be called from the queue using the index of the prev tool\n",
    "            # CONDITION FOR FIRST TOOL\n",
    "            if prev_tool is None:\n",
    "                next_tool = workflow_steps[0][\"pipeline_name\"]\n",
    "                tool_input = workflow_steps[0][\"payload\"]\n",
    "                state.update({\"tool_number\": 0})\n",
    "\n",
    "            # CONDITION FOR LAST TOOL\n",
    "            elif tool_queue.index(prev_tool) == len(tool_queue) - 1:\n",
    "                next_tool = None\n",
    "                print(\"\\nDone servicing the entire queue!\")\n",
    "                return {\n",
    "                    **state,\n",
    "                    \"tool_name\": None,\n",
    "                    \"tool_input\": None,\n",
    "                    \"done\": True\n",
    "                }\n",
    "\n",
    "            # CONDITION FOR A TOOL IN BETWEEN\n",
    "            else:\n",
    "                tool_index = tool_queue.index(prev_tool)\n",
    "                next_tool = workflow_steps[tool_index + 1][\"pipeline_name\"]\n",
    "                tool_input = workflow_steps[tool_index + 1][\"payload\"]\n",
    "\n",
    "            print(f\"\\nTool queue                    : {tool_queue}\")\n",
    "            tool_index = tool_queue.index(next_tool)\n",
    "            print(f\"\\nIndex of next Tool in queue   : {tool_index}\")\n",
    "            print(f\"\\nNext Tool Name                : {next_tool}\")\n",
    "            print(f\"\\nNext Tool Input               : {tool_input}\")\n",
    "\n",
    "            return {\n",
    "                **state,\n",
    "                \"tool_name\": next_tool,\n",
    "                \"tool_input\": tool_input\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"plan_tool_use ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32472b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# interpret_v5: _generate calls static_generate inside of it\n",
    "graph.add_node(\"interpret\", interpret_v6)\n",
    "graph.add_node(\"plan_tool_use\", plan_tool_use_v4)\n",
    "graph.add_node(\"use_tool\", use_tool_v7)  # contains the generate_workflow tool\n",
    "graph.add_node(\"end\", end)\n",
    "\n",
    "graph.set_entry_point(\"interpret\")\n",
    "graph.add_edge(\"interpret\", \"plan_tool_use\")\n",
    "graph.add_edge(\"plan_tool_use\", \"use_tool\")\n",
    "# graph.add_edge(\"use_tool\", \"interpret\")  # optional loop\n",
    "# Forces langgraph to explicitly terminate the flow after reaching the end node\n",
    "graph.add_edge(\"end\", END)\n",
    "graph.add_conditional_edges(\"use_tool\", check_done, {\n",
    "    \"end\": \"end\",\n",
    "    \"interpret\": \"interpret\"\n",
    "})\n",
    "\n",
    "# add the memory as a checkpointed when compiling the graph\n",
    "custom_agent = graph.compile()\n",
    "print(\"Graph compiled and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Share the count of FED Transactions stopped for repair for the last Friday of the month of August 2025\"\n",
    "query2 = \"Hey, what's the weather today?\"\n",
    "query3 = \"How about the first Friday of September?\"\n",
    "\n",
    "response = custom_agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful AI assistant. Use the provided tools to service the user's query.\"),\n",
    "            HumanMessage(content=query1)\n",
    "        ],\n",
    "        \"use_static\": USE_STATIC,\n",
    "        \"done\": False\n",
    "    }\n",
    ")\n",
    "print(\"\\nFinal response:\")\n",
    "ai_msgs = [m for m in response[\"messages\"] if isinstance(m, AIMessage)]\n",
    "last_ai_message = ai_msgs[-1].content\n",
    "print(last_ai_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
