{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Chatbot POC with CFG GenAI - Updated Version\n",
    "\n",
    "This notebook demonstrates a comprehensive chatbot using LangGraph that:\n",
    "1. Takes user queries\n",
    "2. Determines intent and relevance\n",
    "3. Converts natural language to SQL\n",
    "4. Executes SQL queries against Snowflake\n",
    "5. Formats responses back to natural language\n",
    "6. Handles follow-up questions with context\n",
    "7. **NEW**: Returns raw dataframe when formatting fails\n",
    "\n",
    "## Features:\n",
    "- **CFG GenAI Integration**: Uses proprietary LLM instead of OpenAI\n",
    "- **Multi-table Support**: Handles complex queries across related tables\n",
    "- **Context-Aware**: Processes follow-up questions intelligently\n",
    "- **Real Database**: Direct Snowflake connection\n",
    "- **Session Management**: Tracks conversation history\n",
    "- **Robust Error Handling**: Returns structured data when LLM formatting fails\n",
    "- **Table Formatting**: Automatic table formatting for query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in your environment)\n",
    "# !pip install langgraph pandas python-dotenv snowflake-connector-python cfggenaisdk\n",
    "\n",
    "from snowflake.connector import DictCursor\n",
    "import snowflake.connector\n",
    "from cfggenaisdk import CFGGenAIGDK\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, TypedDict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CFG GenAI Integration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CFG GenAI GDK client\n",
    "USECASE_ID = os.getenv(\"CFG_USECASE_ID\", \"chatbot_usecase\")\n",
    "EXPERIMENT_NAME = os.getenv(\"CFG_EXPERIMENT_NAME\", \"langgraph_chatbot\")\n",
    "EXPERIMENT_DESC = os.getenv(\"CFG_EXPERIMENT_DESC\", \"LangGraph chatbot with CFG GenAI\")\n",
    "\n",
    "try:\n",
    "    gdk = CFGGenAIGDK(USECASE_ID, EXPERIMENT_NAME, EXPERIMENT_DESC)\n",
    "    print(\"âœ… CFG GenAI GDK initialized successfully!\")\n",
    "    print(f\"Usecase ID: {USECASE_ID}\")\n",
    "    print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to initialize CFG GenAI GDK: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "def call_llm(prompt: str, system_prompt: str = \"\", model: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Robust helper function to make LLM calls using CFG GenAI GDK\n",
    "    Handles different response structures\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use model from environment variable if not specified\n",
    "        if model is None:\n",
    "            model = os.getenv(\"CFG_MODEL_ID\", \"md005_openai_gpt4o\")\n",
    "        \n",
    "        # Combine system prompt and user prompt\n",
    "        if system_prompt:\n",
    "            combined_prompt = f\"{system_prompt}\\n\\nUser Query: {prompt}\"\n",
    "        else:\n",
    "            combined_prompt = prompt\n",
    "        \n",
    "        # Prepare prompt template for CFG GenAI\n",
    "        prompt_template = {\n",
    "            \"prompt_template\": [\n",
    "                {\"role\": \"system\", \"content\": combined_prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Set hyperparameters\n",
    "        hyperparam = {\n",
    "            'max_tokens': 2000,\n",
    "            'temperature': 0.1\n",
    "        }\n",
    "        \n",
    "        # Make the LLM call\n",
    "        gdk_response = gdk.invoke_llmgateway(\n",
    "            prompt_template=prompt_template,\n",
    "            hyperparam=hyperparam,\n",
    "            model_id=model\n",
    "        )\n",
    "        \n",
    "        # Try different ways to extract the content based on response structure\n",
    "        generated_response = None\n",
    "        \n",
    "        # Method 1: Standard OpenAI-like structure\n",
    "        try:\n",
    "            if isinstance(gdk_response, dict):\n",
    "                generated_response = gdk_response['genResponse']['choices'][0]['message']['content']\n",
    "        except (KeyError, TypeError, IndexError):\n",
    "            pass\n",
    "        \n",
    "        # Method 2: Direct choices access\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, dict) and 'choices' in gdk_response:\n",
    "                    generated_response = gdk_response['choices'][0]['message']['content']\n",
    "            except (KeyError, TypeError, IndexError):\n",
    "                pass\n",
    "        \n",
    "        # Method 3: Direct response field\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, dict):\n",
    "                    if 'response' in gdk_response:\n",
    "                        generated_response = gdk_response['response']\n",
    "                    elif 'content' in gdk_response:\n",
    "                        generated_response = gdk_response['content']\n",
    "                    elif 'text' in gdk_response:\n",
    "                        generated_response = gdk_response['text']\n",
    "            except (KeyError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # Method 4: Tuple/List response\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, (list, tuple)) and len(gdk_response) > 0:\n",
    "                    first_element = gdk_response[0]\n",
    "                    if isinstance(first_element, str):\n",
    "                        generated_response = first_element\n",
    "                    elif isinstance(first_element, dict):\n",
    "                        if 'content' in first_element:\n",
    "                            generated_response = first_element['content']\n",
    "                        elif 'message' in first_element:\n",
    "                            generated_response = first_element['message']\n",
    "                        elif 'text' in first_element:\n",
    "                            generated_response = first_element['text']\n",
    "            except (IndexError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # Method 5: String response\n",
    "        if generated_response is None:\n",
    "            try:\n",
    "                if isinstance(gdk_response, str):\n",
    "                    generated_response = gdk_response\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # If all methods failed, return the raw response as string\n",
    "        if generated_response is None:\n",
    "            generated_response = str(gdk_response)\n",
    "        \n",
    "        return generated_response.strip() if generated_response else \"No response generated\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CFG GenAI LLM call failed: {e}\")\n",
    "        if 'gdk_response' in locals():\n",
    "            print(f\"Response type: {type(gdk_response)}\")\n",
    "            print(f\"Response: {gdk_response}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… CFG GenAI integration setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test CFG GenAI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the CFG GenAI integration\n",
    "test_response = call_llm(\n",
    "    prompt=\"What is 2 + 2?\",\n",
    "    system_prompt=\"You are a helpful assistant. Provide brief, accurate answers.\"\n",
    ")\n",
    "\n",
    "print(f\"ðŸ§ª Test Response: {test_response}\")\n",
    "print(\"âœ… CFG GenAI integration test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Schema and Snowflake Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseSchema:\n",
    "    \"\"\"\n",
    "    Load and manage database schema from external text file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema_file_path: str = \"database_schema.txt\"):\n",
    "        self.schema_file_path = schema_file_path\n",
    "        self.tables = {}\n",
    "        self.relationships = {}\n",
    "        self.raw_schema_text = \"\"\n",
    "        self.load_schema()\n",
    "\n",
    "    def load_schema(self):\n",
    "        \"\"\"\n",
    "        Load database schema from text file - REQUIRED for operation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.schema_file_path):\n",
    "                with open(self.schema_file_path, 'r', encoding='utf-8') as file:\n",
    "                    self.raw_schema_text = file.read()\n",
    "                print(f\"âœ… Database schema loaded from {self.schema_file_path}\")\n",
    "                self.parse_schema()\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Schema file {self.schema_file_path} not found. Please create this file with your database schema.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading schema file: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def parse_schema(self):\n",
    "        \"\"\"\n",
    "        Parse the schema text file and extract table information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lines = self.raw_schema_text.strip().split('\\n')\n",
    "            current_table = None\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                if line.upper().startswith('TABLE:') or line.upper().startswith('TABLE '):\n",
    "                    current_table = line.split(':', 1)[1].strip() if ':' in line else line.split(' ', 1)[1].strip()\n",
    "                    current_table = current_table.upper()\n",
    "                    self.tables[current_table] = {\n",
    "                        'description': f\"Table: {current_table}\",\n",
    "                        'columns': {}\n",
    "                    }\n",
    "\n",
    "                elif current_table and ('|' in line or '\\t' in line or '  ' in line):\n",
    "                    parts = []\n",
    "                    if '|' in line:\n",
    "                        parts = [part.strip() for part in line.split('|')]\n",
    "                    elif '\\t' in line:\n",
    "                        parts = [part.strip() for part in line.split('\\t') if part.strip()]\n",
    "                    else:\n",
    "                        parts = [part.strip() for part in line.split() if part.strip()]\n",
    "\n",
    "                    if len(parts) >= 2:\n",
    "                        col_name = parts[0].upper()\n",
    "                        col_type = parts[1].upper()\n",
    "                        col_description = parts[2] if len(parts) > 2 else f\"{col_name} column\"\n",
    "\n",
    "                        self.tables[current_table]['columns'][col_name] = {\n",
    "                            'type': col_type,\n",
    "                            'description': col_description\n",
    "                        }\n",
    "\n",
    "            print(f\"âœ… Parsed {len(self.tables)} tables from schema file\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error parsing schema: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def get_schema_description(self) -> str:\n",
    "        if self.raw_schema_text and len(self.tables) > 1:\n",
    "            schema_text = \"DATABASE SCHEMA:\\n\\n\"\n",
    "            schema_text += self.raw_schema_text\n",
    "            schema_text += \"\\n\\nAVAILABLE TABLES:\\n\"\n",
    "            for table_name in self.tables.keys():\n",
    "                schema_text += f\"- {table_name}\\n\"\n",
    "            return schema_text\n",
    "        else:\n",
    "            schema_text = \"DATABASE SCHEMA:\\n\\n\"\n",
    "            for table_name, table_info in self.tables.items():\n",
    "                schema_text += f\"Table: {table_name}\\n\"\n",
    "                schema_text += f\"Description: {table_info['description']}\\n\"\n",
    "                schema_text += \"Columns:\\n\"\n",
    "                for col_name, col_info in table_info['columns'].items():\n",
    "                    schema_text += f\"  - {col_name} ({col_info['type']}): {col_info['description']}\\n\"\n",
    "                schema_text += \"\\n\"\n",
    "            return schema_text\n",
    "\n",
    "    def get_table_names(self) -> List[str]:\n",
    "        return list(self.tables.keys())\n",
    "\n",
    "\n",
    "class SnowflakeConfig:\n",
    "    \"\"\"\n",
    "    Snowflake connection configuration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "        self.user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "        self.password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "        self.warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\n",
    "        self.database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "        self.schema = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n",
    "        self.role = os.getenv(\"SNOWFLAKE_ROLE\", \"ACCOUNTADMIN\")\n",
    "\n",
    "        required_fields = {\n",
    "            'account': self.account,\n",
    "            'user': self.user,\n",
    "            'password': self.password,\n",
    "            'database': self.database\n",
    "        }\n",
    "\n",
    "        missing_fields = [field for field, value in required_fields.items() if not value]\n",
    "        if missing_fields:\n",
    "            raise ValueError(\n",
    "                f\"Missing required Snowflake configuration: {', '.join(missing_fields)}. Please check your .env file.\")\n",
    "\n",
    "\n",
    "# Global config instances\n",
    "snowflake_config = None\n",
    "database_schema = None\n",
    "\n",
    "\n",
    "def initialize_configs():\n",
    "    global snowflake_config, database_schema\n",
    "    if snowflake_config is None:\n",
    "        snowflake_config = SnowflakeConfig()\n",
    "    if database_schema is None:\n",
    "        database_schema = DatabaseSchema()\n",
    "    return snowflake_config, database_schema\n",
    "\n",
    "\n",
    "def get_snowflake_connection():\n",
    "    global snowflake_config\n",
    "    if snowflake_config is None:\n",
    "        snowflake_config = SnowflakeConfig()\n",
    "\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            account=snowflake_config.account,\n",
    "            user=snowflake_config.user,\n",
    "            password=snowflake_config.password,\n",
    "            warehouse=snowflake_config.warehouse,\n",
    "            database=snowflake_config.database,\n",
    "            schema=snowflake_config.schema,\n",
    "            role=snowflake_config.role\n",
    "        )\n",
    "        print(\"âœ… Snowflake connection established successfully!\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to connect to Snowflake: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def execute_snowflake_query(sql_query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against Snowflake and return results\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_snowflake_connection()\n",
    "        cursor = conn.cursor(DictCursor)\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "\n",
    "        print(f\"ðŸ“Š Query executed successfully. Returned {len(results)} rows.\")\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": results,\n",
    "            \"columns\": columns,\n",
    "            \"row_count\": len(results)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error executing Snowflake query: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"data\": [],\n",
    "            \"columns\": [],\n",
    "            \"row_count\": 0\n",
    "        }\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "print(\"âœ… Database configuration setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configurations on startup to validate setup\n",
    "try:\n",
    "    initialize_configs()\n",
    "    print(\"âœ… All configurations initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Configuration initialization failed: {e}\")\n",
    "    print(\"Please check your .env file and database_schema.txt file before running queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. State Management and Session Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"\n",
    "    Represents a single conversation turn (query + response)\n",
    "    \"\"\"\n",
    "    query: str\n",
    "    response: str\n",
    "    sql_query: str\n",
    "    query_result: Dict[str, Any]\n",
    "    timestamp: datetime\n",
    "    intent: str\n",
    "\n",
    "\n",
    "class SessionManager:\n",
    "    \"\"\"\n",
    "    Manages conversation sessions and context for follow-up questions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sessions: Dict[str, List[ConversationTurn]] = {}\n",
    "        self.max_history = 5  # Keep last 5 conversation turns\n",
    "\n",
    "    def add_turn(self, session_id: str, turn: ConversationTurn):\n",
    "        \"\"\"Add a conversation turn to the session history\"\"\"\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = []\n",
    "\n",
    "        self.sessions[session_id].append(turn)\n",
    "\n",
    "        # Keep only the last max_history turns\n",
    "        if len(self.sessions[session_id]) > self.max_history:\n",
    "            self.sessions[session_id] = self.sessions[session_id][-self.max_history:]\n",
    "\n",
    "    def get_conversation_context(self, session_id: str) -> str:\n",
    "        \"\"\"Get formatted conversation context for LLM\"\"\"\n",
    "        if session_id not in self.sessions or not self.sessions[session_id]:\n",
    "            return \"No previous conversation history.\"\n",
    "\n",
    "        context = \"PREVIOUS CONVERSATION HISTORY:\\n\"\n",
    "        for i, turn in enumerate(self.sessions[session_id], 1):\n",
    "            context += f\"\\n{i}. User: {turn.query}\"\n",
    "            context += f\"\\n   Response: {turn.response}\"\n",
    "            if turn.sql_query:\n",
    "                context += f\"\\n   SQL Used: {turn.sql_query}\"\n",
    "\n",
    "        context += \"\\n\\nUse this context to understand follow-up questions and references like 'that', 'those', 'compare to previous', etc.\"\n",
    "        return context\n",
    "\n",
    "    def clear_session(self, session_id: str):\n",
    "        \"\"\"Clear session history\"\"\"\n",
    "        if session_id in self.sessions:\n",
    "            del self.sessions[session_id]\n",
    "\n",
    "\n",
    "class WorkflowState(TypedDict):\n",
    "    \"\"\"\n",
    "    Enhanced state structure for our LangGraph workflow with conversation context\n",
    "    \"\"\"\n",
    "    session_id: str\n",
    "    user_query: str\n",
    "    original_query: str\n",
    "    intent: str\n",
    "    is_relevant: bool\n",
    "    is_followup: bool\n",
    "    sql_query: str\n",
    "    query_result: Dict[str, Any]\n",
    "    final_response: str\n",
    "    error: str\n",
    "    context: Dict[str, Any]\n",
    "    conversation_context: str\n",
    "\n",
    "\n",
    "# Global session manager\n",
    "session_manager = SessionManager()\n",
    "\n",
    "\n",
    "def initialize_state(session_id: str, user_query: str) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Initialize a new workflow state with conversation context\n",
    "    \"\"\"\n",
    "    conversation_context = session_manager.get_conversation_context(session_id)\n",
    "\n",
    "    return WorkflowState(\n",
    "        session_id=session_id,\n",
    "        user_query=user_query,\n",
    "        original_query=user_query,\n",
    "        intent=\"\",\n",
    "        is_relevant=False,\n",
    "        is_followup=False,\n",
    "        sql_query=\"\",\n",
    "        query_result={},\n",
    "        final_response=\"\",\n",
    "        error=\"\",\n",
    "        context={},\n",
    "        conversation_context=conversation_context\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"âœ… State management and session handling setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tool Functions - Core Processing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enhancer_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Tool 0: Enhance user query with conversation context for follow-up questions\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”— Enhancing query with context: {state['user_query']}\")\n",
    "\n",
    "    # Check if there's previous conversation context\n",
    "    if state['conversation_context'] == \"No previous conversation history.\":\n",
    "        print(\"âœ… No previous context - using original query\")\n",
    "        return state\n",
    "\n",
    "    # Use LLM to determine if this is a follow-up and enhance the query\n",
    "    system_prompt = f\"\"\"You are a context enhancement assistant for a business data analysis chatbot.\n",
    "\n",
    "{state['conversation_context']}\n",
    "\n",
    "Your job is to:\n",
    "1. Determine if the current user query is a follow-up question that references previous conversation\n",
    "2. If it's a follow-up, enhance the query with proper context to make it standalone\n",
    "3. If it's not a follow-up, return the original query unchanged\n",
    "\n",
    "Follow-up indicators include:\n",
    "- References like \"that\", \"those\", \"it\", \"them\"\n",
    "- Comparative phrases like \"compared to that\", \"vs the previous\", \"how about\"\n",
    "- Continuation phrases like \"what about\", \"and for\", \"also show me\"\n",
    "- Time references building on previous queries like \"and last month\", \"for the same period\"\n",
    "\n",
    "IMPORTANT: \n",
    "- If it's a follow-up, rewrite the query to be completely standalone and clear\n",
    "- If it's NOT a follow-up, return exactly: \"NOT_FOLLOWUP: [original query]\"\n",
    "- If it IS a follow-up, return: \"FOLLOWUP: [enhanced standalone query]\"\n",
    "\n",
    "Examples:\n",
    "- \"What about premium customers?\" â†’ \"FOLLOWUP: What are the sales metrics for premium customers?\"\n",
    "- \"Compare that to last month\" â†’ \"FOLLOWUP: Compare the transaction volume from the previous query to last month's transaction volume\"\n",
    "- \"Show me product sales\" â†’ \"NOT_FOLLOWUP: Show me product sales\"\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Current user query: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        print(f\"ðŸ¤– Context Enhancement Response: {llm_response}\")\n",
    "\n",
    "        if llm_response.startswith(\"FOLLOWUP:\"):\n",
    "            # This is a follow-up question - use enhanced query\n",
    "            enhanced_query = llm_response.replace(\"FOLLOWUP:\", \"\").strip()\n",
    "            state['user_query'] = enhanced_query\n",
    "            state['is_followup'] = True\n",
    "            print(f\"âœ… Enhanced follow-up query: {enhanced_query}\")\n",
    "        else:\n",
    "            # Not a follow-up - keep original query\n",
    "            state['is_followup'] = False\n",
    "            print(\"âœ… Not a follow-up question - using original query\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error in context enhancement: {str(e)}\"\n",
    "        print(f\"âŒ Error in context enhancement: {e}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def intent_classifier_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Tool 1: Determine if the user query is relevant to our use case using LLM\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Analyzing intent for: {state['user_query']}\")\n",
    "\n",
    "    system_prompt = \"\"\"You are an intent classifier for a comprehensive business data analysis system.\n",
    "    \n",
    "    Your job is to determine if a user query is relevant to business data analysis using our available tables:\n",
    "    - TRANSACTIONS: Transaction records with volumes, dates, customer and product IDs\n",
    "    - CUSTOMERS: Customer information including segments, locations, registration dates\n",
    "    - PRODUCTS: Product catalog with categories, prices, brands, suppliers\n",
    "    \n",
    "    Relevant queries include:\n",
    "    - Transaction analysis (volumes, amounts, counts, trends)\n",
    "    - Customer analysis (segments, behavior, demographics, registration patterns)\n",
    "    - Product analysis (categories, performance, pricing, brand analysis)\n",
    "    - Sales and revenue analysis across any dimension\n",
    "    - Time-based queries (last Friday, yesterday, this week, monthly trends, etc.)\n",
    "    - Comparative analysis (compare periods, segments, products, customers)\n",
    "    - Cross-table analysis (customer transaction patterns, product performance by segment, etc.)\n",
    "    - Data aggregation requests (total, sum, average, count, etc.)\n",
    "    - Business intelligence queries combining multiple data sources\n",
    "    \n",
    "    Irrelevant queries include:\n",
    "    - General conversation, greetings\n",
    "    - Questions about weather, news, personal topics\n",
    "    - Technical support unrelated to data\n",
    "    - Requests for information outside of our business data domain\n",
    "    \n",
    "    Respond with ONLY one of these formats:\n",
    "    RELEVANT: data_query\n",
    "    IRRELEVANT: general_conversation\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Classify this user query: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        print(f\"ðŸ¤– LLM Intent Response: {llm_response}\")\n",
    "\n",
    "        if \"RELEVANT\" in llm_response.upper():\n",
    "            state['intent'] = \"data_query\"\n",
    "            state['is_relevant'] = True\n",
    "            print(\"âœ… Query is relevant - proceeding with data analysis\")\n",
    "        else:\n",
    "            state['intent'] = \"irrelevant\"\n",
    "            state['is_relevant'] = False\n",
    "            state['final_response'] = \"I'm sorry, but I can only help with transaction and data-related queries. Please ask about transaction volumes, sales data, or similar topics.\"\n",
    "            print(\"âŒ Query is not relevant to our use case\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error in intent classification: {str(e)}\"\n",
    "        print(f\"âŒ Error in intent classification: {e}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"âœ… Context and Intent tools defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_to_sql_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Tool 2: Convert natural language to SQL using LLM with multi-table support\n",
    "    \"\"\"\n",
    "    if not state['is_relevant']:\n",
    "        return state\n",
    "\n",
    "    print(f\"ðŸ”„ Converting to SQL: {state['user_query']}\")\n",
    "\n",
    "    # Get current date for context\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    current_day = datetime.now().strftime('%A')\n",
    "\n",
    "    # Initialize configs and get the comprehensive schema description\n",
    "    global snowflake_config, database_schema\n",
    "    if snowflake_config is None:\n",
    "        snowflake_config = SnowflakeConfig()\n",
    "    if database_schema is None:\n",
    "        database_schema = DatabaseSchema()\n",
    "\n",
    "    schema_description = database_schema.get_schema_description()\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert SQL query generator for a Snowflake database with multiple related tables.\n",
    "\n",
    "{schema_description}\n",
    "\n",
    "Database Configuration:\n",
    "- Database: {snowflake_config.database}\n",
    "- Schema: {snowflake_config.schema}\n",
    "- Current date: {current_date} ({current_day})\n",
    "\n",
    "SNOWFLAKE SQL GENERATION RULES:\n",
    "\n",
    "1. QUERY STRUCTURE:\n",
    "   - Generate ONLY the SQL query, no explanations or markdown\n",
    "   - Use fully qualified table names: {snowflake_config.database}.{snowflake_config.schema}.TABLE_NAME\n",
    "   - Always use uppercase for SQL keywords and table/column names\n",
    "\n",
    "2. MULTI-TABLE QUERIES:\n",
    "   - Use JOINs when the query requires data from multiple tables\n",
    "   - Common patterns:\n",
    "     * Customer analysis: JOIN TRANSACTIONS with CUSTOMERS\n",
    "     * Product analysis: JOIN TRANSACTIONS with PRODUCTS  \n",
    "     * Complete analysis: JOIN all three tables\n",
    "   - Use appropriate JOIN types (INNER JOIN for existing relationships)\n",
    "\n",
    "3. DATE FUNCTIONS (Snowflake-specific):\n",
    "   - Current date: CURRENT_DATE()\n",
    "   - Yesterday: DATEADD('day', -1, CURRENT_DATE())\n",
    "   - Last Friday: DATEADD('day', -1, DATE_TRUNC('week', CURRENT_DATE()) + 4)\n",
    "   - Last week: DATE_TRUNC('week', DATEADD('week', -1, CURRENT_DATE()))\n",
    "   - This month: DATE_TRUNC('month', CURRENT_DATE())\n",
    "   - Last month: DATE_TRUNC('month', DATEADD('month', -1, CURRENT_DATE()))\n",
    "\n",
    "4. AGGREGATIONS:\n",
    "   - Use SUM() for volume/amount calculations\n",
    "   - Use COUNT() for transaction counts\n",
    "   - Use AVG() for averages\n",
    "   - Use GROUP BY for breakdowns by categories, dates, etc.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Convert this natural language query to SQL: '{state['user_query']}'\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        # Clean up the response to extract just the SQL\n",
    "        sql_query = llm_response.strip()\n",
    "\n",
    "        # Remove any markdown formatting or extra text\n",
    "        if \"```sql\" in sql_query:\n",
    "            sql_query = sql_query.split(\"```sql\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in sql_query:\n",
    "            sql_query = sql_query.split(\"```\")[1].strip()\n",
    "\n",
    "        # Remove any trailing semicolon and clean up\n",
    "        sql_query = sql_query.rstrip(';').strip()\n",
    "\n",
    "        state['sql_query'] = sql_query\n",
    "        print(f\"ðŸ“ Generated SQL: {state['sql_query']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error generating SQL: {str(e)}\"\n",
    "        print(f\"âŒ Error in SQL generation: {e}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def sql_executor_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Tool 3: Execute the generated SQL query against Snowflake database\n",
    "    \"\"\"\n",
    "    if not state['is_relevant'] or state['error'] or not state['sql_query']:\n",
    "        return state\n",
    "\n",
    "    print(f\"âš¡ Executing SQL against Snowflake: {state['sql_query']}\")\n",
    "\n",
    "    try:\n",
    "        # Execute the query against Snowflake\n",
    "        query_result = execute_snowflake_query(state['sql_query'])\n",
    "\n",
    "        if query_result['success']:\n",
    "            # Process the results into a format suitable for response generation\n",
    "            data = query_result['data']\n",
    "\n",
    "            if not data:\n",
    "                state['query_result'] = {'message': 'No data found for the specified criteria'}\n",
    "            elif len(data) == 1:\n",
    "                # Single row result\n",
    "                state['query_result'] = dict(data[0])\n",
    "            else:\n",
    "                # Multiple rows - convert to a more readable format\n",
    "                if len(data) <= 10:  # For small result sets, include all data\n",
    "                    state['query_result'] = {\n",
    "                        'results': [dict(row) for row in data],\n",
    "                        'row_count': len(data)\n",
    "                    }\n",
    "                else:\n",
    "                    # For large result sets, provide summary\n",
    "                    state['query_result'] = {\n",
    "                        'sample_results': [dict(row) for row in data[:5]],\n",
    "                        'total_rows': len(data),\n",
    "                        'message': f'Showing first 5 of {len(data)} results'\n",
    "                    }\n",
    "\n",
    "            print(f\"ðŸ“Š Query executed successfully. Result: {state['query_result']}\")\n",
    "\n",
    "        else:\n",
    "            # Query failed\n",
    "            state['error'] = f\"Snowflake query failed: {query_result['error']}\"\n",
    "            print(f\"âŒ Snowflake query failed: {query_result['error']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        state['error'] = f\"Error executing SQL: {str(e)}\"\n",
    "        print(f\"âŒ Error in SQL execution: {e}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"âœ… SQL generation and execution tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Enhanced Response Formatter with Table Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results_as_table(results: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format query results as a simple table string\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return \"No data available\"\n",
    "\n",
    "    try:\n",
    "        # Get all unique keys from all results\n",
    "        all_keys = set()\n",
    "        for result in results:\n",
    "            all_keys.update(result.keys())\n",
    "\n",
    "        headers = list(all_keys)\n",
    "\n",
    "        # Create header row\n",
    "        table_str = \" | \".join(headers) + \"\\n\"\n",
    "        table_str += \"-\" * len(table_str) + \"\\n\"\n",
    "\n",
    "        # Add data rows\n",
    "        for result in results:\n",
    "            row_values = []\n",
    "            for header in headers:\n",
    "                value = result.get(header, \"\")\n",
    "                # Format numbers with commas if they're large\n",
    "                if isinstance(value, (int, float)) and abs(value) >= 1000:\n",
    "                    value = f\"{value:,}\"\n",
    "                row_values.append(str(value))\n",
    "            table_str += \" | \".join(row_values) + \"\\n\"\n",
    "\n",
    "        return table_str\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback to simple string representation\n",
    "        return \"\\n\".join([str(result) for result in results])\n",
    "\n",
    "\n",
    "def format_single_result(result: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format a single result dictionary as a readable string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        formatted_lines = []\n",
    "        for key, value in result.items():\n",
    "            # Format numbers with commas if they're large\n",
    "            if isinstance(value, (int, float)) and abs(value) >= 1000:\n",
    "                value = f\"{value:,}\"\n",
    "            formatted_lines.append(f\"{key}: {value}\")\n",
    "\n",
    "        return \"\\n\".join(formatted_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        return str(result)\n",
    "\n",
    "\n",
    "def response_formatter_tool(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Tool 4: Format the query results into natural language response using LLM\n",
    "    If formatting fails, return the raw dataframe/results\n",
    "    \"\"\"\n",
    "    if not state['is_relevant'] or state['error']:\n",
    "        return state\n",
    "\n",
    "    print(\"ðŸ“ Formatting response to natural language\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a data analyst assistant that converts query results into clear, natural language responses.\n",
    "\n",
    "Guidelines:\n",
    "1. Be conversational and helpful\n",
    "2. Include specific numbers with proper formatting (commas for thousands)\n",
    "3. For comparisons, calculate and mention percentage changes\n",
    "4. Use clear date references\n",
    "5. Keep responses concise but informative\n",
    "6. If showing multiple data points, organize them clearly\n",
    "\n",
    "Examples:\n",
    "- Single value: \"The total transaction volume for last Friday was 123,456.\"\n",
    "- Comparison: \"Transaction volume increased from 110,000 on August 22nd to 123,456 on August 29th, representing a 12.2% increase.\"\n",
    "- Multiple values: \"Here are the recent transaction volumes: August 29th: 123,456, August 28th: 98,765\"\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Prepare the data context for the LLM\n",
    "        data_context = f\"\"\"\n",
    "Original Query: {state['user_query']}\n",
    "SQL Query Used: {state['sql_query']}\n",
    "Query Results: {json.dumps(state['query_result'], indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        llm_response = call_llm(\n",
    "            prompt=f\"Convert this query result into a natural language response:\\n\\n{data_context}\",\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        # Check if the LLM response is valid and not an error\n",
    "        if llm_response and not llm_response.startswith(\"Error:\") and len(llm_response.strip()) > 0:\n",
    "            state['final_response'] = llm_response.strip()\n",
    "            print(f\"âœ… Final response: {state['final_response']}\")\n",
    "        else:\n",
    "            raise ValueError(\"LLM returned invalid or empty response\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in response formatting: {e}\")\n",
    "        print(\"ðŸ“Š Returning raw data instead of formatted response\")\n",
    "\n",
    "        # Return the raw dataframe/results when formatting fails\n",
    "        query_result = state.get('query_result', {})\n",
    "\n",
    "        if isinstance(query_result, dict):\n",
    "            if 'results' in query_result:\n",
    "                # Multiple rows - convert to DataFrame-like string representation\n",
    "                results = query_result['results']\n",
    "                if results:\n",
    "                    # Create a simple table representation\n",
    "                    df_str = format_results_as_table(results)\n",
    "                    state['final_response'] = f\"Here are the query results:\\n\\n{df_str}\"\n",
    "                else:\n",
    "                    state['final_response'] = \"No data found for your query.\"\n",
    "            elif 'sample_results' in query_result:\n",
    "                # Large result set - show sample\n",
    "                sample_results = query_result['sample_results']\n",
    "                total_rows = query_result.get('total_rows', len(sample_results))\n",
    "                df_str = format_results_as_table(sample_results)\n",
    "                state['final_response'] = f\"Here are the first {len(sample_results)} results out of {total_rows} total:\\n\\n{df_str}\"\n",
    "            elif 'message' in query_result:\n",
    "                # No data message\n",
    "                state['final_response'] = query_result['message']\n",
    "            else:\n",
    "                # Single row or simple result\n",
    "                df_str = format_single_result(query_result)\n",
    "                state['final_response'] = f\"Query result:\\n\\n{df_str}\"\n",
    "        else:\n",
    "            # Fallback for unexpected result format\n",
    "            state['final_response'] = f\"Query completed. Result: {str(query_result)}\"\n",
    "\n",
    "        print(f\"ðŸ“‹ Raw data response: {state['final_response'][:200]}...\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"âœ… Enhanced response formatter with table support defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LangGraph Workflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"\n",
    "    Create and configure the LangGraph workflow\n",
    "    \"\"\"\n",
    "    # Create a new state graph\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "\n",
    "    # Add nodes (our tool functions)\n",
    "    workflow.add_node(\"context_enhancer\", context_enhancer_tool)\n",
    "    workflow.add_node(\"intent_classifier\", intent_classifier_tool)\n",
    "    workflow.add_node(\"nl_to_sql\", nl_to_sql_tool)\n",
    "    workflow.add_node(\"sql_executor\", sql_executor_tool)\n",
    "    workflow.add_node(\"response_formatter\", response_formatter_tool)\n",
    "\n",
    "    # Define the workflow edges (execution order)\n",
    "    workflow.set_entry_point(\"context_enhancer\")\n",
    "\n",
    "    # Add conditional logic\n",
    "    def after_context_enhancement(state: WorkflowState) -> str:\n",
    "        \"\"\"Move to intent classification after context enhancement\"\"\"\n",
    "        if state['error']:\n",
    "            return END\n",
    "        else:\n",
    "            return \"intent_classifier\"\n",
    "\n",
    "    def should_continue(state: WorkflowState) -> str:\n",
    "        \"\"\"Decide whether to continue processing or end\"\"\"\n",
    "        if not state['is_relevant']:\n",
    "            return END\n",
    "        elif state['error']:\n",
    "            return END\n",
    "        else:\n",
    "            return \"nl_to_sql\"\n",
    "\n",
    "    def after_sql_generation(state: WorkflowState) -> str:\n",
    "        \"\"\"Continue to SQL execution if no errors\"\"\"\n",
    "        if state['error'] or not state['sql_query']:\n",
    "            return END\n",
    "        else:\n",
    "            return \"sql_executor\"\n",
    "\n",
    "    def after_sql_execution(state: WorkflowState) -> str:\n",
    "        \"\"\"Continue to response formatting if no errors\"\"\"\n",
    "        if state['error']:\n",
    "            return END\n",
    "        else:\n",
    "            return \"response_formatter\"\n",
    "\n",
    "    # Add conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"context_enhancer\",\n",
    "        after_context_enhancement,\n",
    "        {\n",
    "            \"intent_classifier\": \"intent_classifier\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"intent_classifier\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"nl_to_sql\": \"nl_to_sql\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"nl_to_sql\",\n",
    "        after_sql_generation,\n",
    "        {\n",
    "            \"sql_executor\": \"sql_executor\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"sql_executor\",\n",
    "        after_sql_execution,\n",
    "        {\n",
    "            \"response_formatter\": \"response_formatter\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Response formatter always ends the workflow\n",
    "    workflow.add_edge(\"response_formatter\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "\n",
    "# Create our workflow\n",
    "chatbot_workflow = create_workflow()\n",
    "print(\"âœ… LangGraph workflow created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Main Chatbot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query: str, session_id: str = \"default\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to process a user query through our LangGraph workflow\n",
    "\n",
    "    Args:\n",
    "        user_query: The user's natural language query\n",
    "        session_id: Session identifier for context management\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the final response and execution details\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸš€ Processing query: '{user_query}'\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize state\n",
    "    initial_state = initialize_state(session_id, user_query)\n",
    "\n",
    "    try:\n",
    "        # Run the workflow\n",
    "        final_state = chatbot_workflow.invoke(initial_state)\n",
    "\n",
    "        # Prepare response\n",
    "        response = {\n",
    "            \"query\": user_query,\n",
    "            \"original_query\": final_state.get('original_query', user_query),\n",
    "            \"enhanced_query\": final_state.get('user_query', user_query),\n",
    "            \"is_followup\": final_state.get('is_followup', False),\n",
    "            \"response\": final_state.get('final_response', 'No response generated'),\n",
    "            \"session_id\": session_id,\n",
    "            \"success\": not bool(final_state.get('error')),\n",
    "            \"error\": final_state.get('error', ''),\n",
    "            \"execution_details\": {\n",
    "                \"intent\": final_state.get('intent', ''),\n",
    "                \"is_relevant\": final_state.get('is_relevant', False),\n",
    "                \"sql_query\": final_state.get('sql_query', ''),\n",
    "                \"query_result\": final_state.get('query_result', {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save conversation turn to session history (only if successful and relevant)\n",
    "        if response['success'] and final_state.get('is_relevant', False):\n",
    "            conversation_turn = ConversationTurn(\n",
    "                query=final_state.get('original_query', user_query),\n",
    "                response=final_state.get('final_response', ''),\n",
    "                sql_query=final_state.get('sql_query', ''),\n",
    "                query_result=final_state.get('query_result', {}),\n",
    "                timestamp=datetime.now(),\n",
    "                intent=final_state.get('intent', '')\n",
    "            )\n",
    "            session_manager.add_turn(session_id, conversation_turn)\n",
    "            print(f\"ðŸ’¾ Saved conversation turn to session {session_id}\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"âœ… Final Response: {response['response']}\")\n",
    "        if response['is_followup']:\n",
    "            print(f\"ðŸ”— Follow-up detected - Enhanced query: {response['enhanced_query']}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        error_response = {\n",
    "            \"query\": user_query,\n",
    "            \"response\": \"I encountered an error while processing your request.\",\n",
    "            \"session_id\": session_id,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"execution_details\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"âŒ Error processing query: {e}\")\n",
    "        return error_response\n",
    "\n",
    "\n",
    "def clear_session(session_id: str):\n",
    "    \"\"\"\n",
    "    Clear conversation history for a session\n",
    "    \"\"\"\n",
    "    session_manager.clear_session(session_id)\n",
    "    print(f\"ðŸ—‘ï¸ Cleared session: {session_id}\")\n",
    "\n",
    "\n",
    "print(\"âœ… Main chatbot function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the Enhanced Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries - uncomment and run these to test your chatbot\n",
    "\n",
    "# Test 1: Simple query\n",
    "# result1 = process_query(\"What was the transaction volume last Friday?\", \"test_session\")\n",
    "# print(f\"Response: {result1['response']}\")\n",
    "\n",
    "# Test 2: Follow-up question\n",
    "# result2 = process_query(\"How does that compare to the previous week?\", \"test_session\")\n",
    "# print(f\"Response: {result2['response']}\")\n",
    "\n",
    "# Test 3: Customer analysis (might trigger table formatting)\n",
    "# result3 = process_query(\"Show me sales by customer segment this month\", \"test_session\")\n",
    "# print(f\"Response: {result3['response']}\")\n",
    "\n",
    "# Test 4: Irrelevant query\n",
    "# result4 = process_query(\"What's the weather like today?\", \"test_session\")\n",
    "# print(f\"Response: {result4['response']}\")\n",
    "\n",
    "print(\"ðŸ§ª Test queries are ready to run!\")\n",
    "print(\"Uncomment the test queries above to try them out.\")\n",
    "print(\"\\nðŸŽ¯ Key Features:\")\n",
    "print(\"- âœ… CFG GenAI Integration\")\n",
    "print(\"- âœ… Context-aware follow-up questions\")\n",
    "print(\"- âœ… Multi-table SQL generation\")\n",
    "print(\"- âœ… Robust error handling\")\n",
    "print(\"- âœ… Automatic table formatting when LLM formatting fails\")\n",
    "print(\"- âœ… Session management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_demo():\n",
    "    \"\"\"\n",
    "    Interactive demo function for testing the chatbot\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ‰ Welcome to the Enhanced LangGraph Chatbot with CFG GenAI!\")\n",
    "    print(\"âœ¨ NEW: Automatic table formatting when LLM formatting fails\")\n",
    "    print(\"Type 'quit' to exit, 'clear' to clear session history\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    session_id = \"interactive_session\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nðŸ’¬ You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"ðŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'clear':\n",
    "                clear_session(session_id)\n",
    "                continue\n",
    "            elif not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Process the query\n",
    "            result = process_query(user_input, session_id)\n",
    "            \n",
    "            # Display the response\n",
    "            print(f\"\\nðŸ¤– Bot: {result['response']}\")\n",
    "            \n",
    "            # Show additional details if there was an error\n",
    "            if not result['success'] and result['error']:\n",
    "                print(f\"âŒ Error: {result['error']}\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Unexpected error: {e}\")\n",
    "\n",
    "# Uncomment the line below to start the interactive demo\n",
    "# interactive_demo()\n",
    "\n",
    "print(\"ðŸŽ¯ Interactive demo function ready!\")\n",
    "print(\"Call interactive_demo() to start chatting with the enhanced bot.\")\n",
    "print(\"\\nðŸ“Š The bot will now automatically format results as tables when LLM formatting fails!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}